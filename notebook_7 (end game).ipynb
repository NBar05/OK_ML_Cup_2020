{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "srmZrir30OL5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import string\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "ZzKSezAp0OMJ",
    "outputId": "93852034-0e37-4273-c11d-2de43a378543",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_lemmatised</th>\n",
       "      <th>normal</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>obscenity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41127</td>\n",
       "      <td>–¥–≤–æ—Ä–Ω–∏–∫–∞ –Ω–∞–¥–æ —Ç–æ–∂–µ —É–Ω–∏—á—Ç–æ–∂–∏—Ç—å!</td>\n",
       "      <td>–¥–≤–æ—Ä–Ω–∏–∫ –Ω–∞–¥–æ —Ç–æ–∂–µ —É–Ω–∏—á—Ç–æ–∂</td>\n",
       "      <td>–¥–≤–æ—Ä–Ω–∏–∫ –Ω–∞–¥–æ —Ç–æ–∂–µ —É–Ω–∏—á—Ç–æ–∂–∏—Ç—å</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6812</td>\n",
       "      <td>–º–æ—è —Å—Ç–∞—Ä—à–∞—è –Ω–µ–¥–µ–ª—é —à–∏–ø–µ–ª–∞, –Ω–µ –ø—Ä–∏–Ω–∏–º–∞–ª–∞ –ø–æ–¥–∫–∏–¥...</td>\n",
       "      <td>–º–æ—è —Å—Ç–∞—Ä—à –Ω–µ–¥–µ–ª —à–∏–ø–µ–ª –Ω–µ –ø—Ä–∏–Ω–∏–º–∞ –ø–æ–¥–∫–∏–¥—ã—à –∫–æ—Ç–æ...</td>\n",
       "      <td>–º–æ–π —Å—Ç–∞—Ä—à–∏–π –Ω–µ–¥–µ–ª—è —à–∏–ø–µ—Ç—å –Ω–µ –ø—Ä–∏–Ω–∏–º–∞—Ç—å –ø–æ–¥–∫–∏–¥—ã...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6256</td>\n",
       "      <td>–ø–æ–ª–Ω–æ—Å—Ç—å—é —Å –≤–∞–º–∏ —Å–æ–≥–ª–∞—Å–Ω–∞!</td>\n",
       "      <td>–ø–æ–ª–Ω–æ—Å—Ç —Å –≤–∞–º —Å–æ–≥–ª–∞—Å–Ω</td>\n",
       "      <td>–ø–æ–ª–Ω–æ—Å—Ç—å—é —Å –≤—ã —Å–æ–≥–ª–∞—Å–Ω—ã–π</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  \\\n",
       "0  41127                     –¥–≤–æ—Ä–Ω–∏–∫–∞ –Ω–∞–¥–æ —Ç–æ–∂–µ —É–Ω–∏—á—Ç–æ–∂–∏—Ç—å!   \n",
       "1   6812  –º–æ—è —Å—Ç–∞—Ä—à–∞—è –Ω–µ–¥–µ–ª—é —à–∏–ø–µ–ª–∞, –Ω–µ –ø—Ä–∏–Ω–∏–º–∞–ª–∞ –ø–æ–¥–∫–∏–¥...   \n",
       "2   6256                         –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å –≤–∞–º–∏ —Å–æ–≥–ª–∞—Å–Ω–∞!   \n",
       "\n",
       "                                        text_stemmed  \\\n",
       "0                          –¥–≤–æ—Ä–Ω–∏–∫ –Ω–∞–¥–æ —Ç–æ–∂–µ —É–Ω–∏—á—Ç–æ–∂   \n",
       "1  –º–æ—è —Å—Ç–∞—Ä—à –Ω–µ–¥–µ–ª —à–∏–ø–µ–ª –Ω–µ –ø—Ä–∏–Ω–∏–º–∞ –ø–æ–¥–∫–∏–¥—ã—à –∫–æ—Ç–æ...   \n",
       "2                              –ø–æ–ª–Ω–æ—Å—Ç —Å –≤–∞–º —Å–æ–≥–ª–∞—Å–Ω   \n",
       "\n",
       "                                     text_lemmatised  normal  threat  insult  \\\n",
       "0                       –¥–≤–æ—Ä–Ω–∏–∫ –Ω–∞–¥–æ —Ç–æ–∂–µ —É–Ω–∏—á—Ç–æ–∂–∏—Ç—å       0       1       0   \n",
       "1  –º–æ–π —Å—Ç–∞—Ä—à–∏–π –Ω–µ–¥–µ–ª—è —à–∏–ø–µ—Ç—å –Ω–µ –ø—Ä–∏–Ω–∏–º–∞—Ç—å –ø–æ–¥–∫–∏–¥—ã...       1       0       0   \n",
       "2                           –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å –≤—ã —Å–æ–≥–ª–∞—Å–Ω—ã–π       1       0       0   \n",
       "\n",
       "   obscenity  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY = pd.read_csv('XY.csv', header = 0)\n",
    "XY.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4KCI2T-A0OMT",
    "outputId": "f130e329-2974-4ac2-f735-629b64c0cbdd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((104142, 10), (18547, 10))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY['exclamation_num'] = XY.text.str.count('!')\n",
    "XY['question_num'] = XY.text.str.count('\\?')\n",
    "\n",
    "XY_train, XY_test = train_test_split(XY, test_size = 0.3, shuffle = True, random_state = 42)\n",
    "XY_train.reset_index(drop = True, inplace = True)\n",
    "XY_test.reset_index(drop = True, inplace = True)\n",
    "XY_train_abn = XY_train.loc[XY_train.normal == 0, :].reset_index(drop = True)\n",
    "XY_train.shape, XY_train_abn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['–±—ã–¥–ª–æ',\n",
    " '–¥—Ä–æ—á–∏—Ç—å',\n",
    " '–∫–æ–Ω—á–∏—Ç—å',\n",
    " '–æ—Ç—Ä–µ–∑–∞—Ç—å',\n",
    " '–µ–±–∞–ª',\n",
    " '–ø–∏–∑–¥–µ—Ü',\n",
    " '–≤–¥—É—Ç—å',\n",
    " '–ø–æ–ª–∏–∑–∞—Ç—å',\n",
    " '—Å–æ—Å–Ω—É—Ç—å',\n",
    " '–ø–æ–≤–µ—Å–∏—Ç—å',\n",
    " '—É—Ç–æ–ø–∏—Ç—å',\n",
    " '–ø–∏–∑–¥–∞',\n",
    " '—Å–ø–µ—Ä–º–∞',\n",
    " '–∫–æ–∑—ë–ª',\n",
    " '–ø–∏–¥–æ—Ä–∞—Å',\n",
    " '–Ω–∞–±–∏—Ç—å',\n",
    " '–≥–æ–≤–Ω–æ',\n",
    " '–∑–∞–∫–æ–ø–∞—Ç—å',\n",
    " '—Ö–æ—Ö–æ–ª',\n",
    " '–≤–æ—Ä',\n",
    " '–¥—É—Ä–∞',\n",
    " '—É–±–ª—é–¥–æ–∫',\n",
    " '–∫–∞–∑–Ω–∏—Ç—å',\n",
    " '–≤—ã–µ–±–∞–ª',\n",
    " '–ª–∏–∑–∞—Ç—å',\n",
    " '–∑–∞—Å–∞–¥–∏—Ç—å',\n",
    " '—á–º–æ',\n",
    " '—Ä–∞—Å—Ç—Ä–µ–ª–∞',\n",
    " '–∑–∞–¥–Ω–∏—Ü–∞',\n",
    " '–ø—Ä–∏–ª—é–¥–Ω–æ',\n",
    " '–≥–Ω–∏–¥–∞',\n",
    " '—Å—É–∫–∞',\n",
    " '—Ä–∞—Å—Ç—Ä–µ–ª—è—Ç—å',\n",
    " '–Ω–∞—Å–æ—Å–∞—Ç—å',\n",
    " '—Å–æ—Å–∞—Ç—å —Å–æ—Å–∞—Ç—å',\n",
    " '–ø–∏–∑–¥–∏—Ç—å',\n",
    " '–ø—Ä–æ–∫–ª—è—Ç—ã–π',\n",
    " '–±–∏—Ç—å',\n",
    " '–æ—Ç—Å—Ç—Ä–µ–ª–∏–≤–∞—Ç—å',\n",
    " '–µ–±—É',\n",
    " '—É–±–∏–≤–∞—Ç—å',\n",
    " '–∫–æ–Ω—á–µ–Ω—ã–π',\n",
    " '–∂–∏–≤–æ—Ç–Ω–æ–µ',\n",
    " '—Å–µ–∫—Å',\n",
    " '–±–ª—è–¥—å',\n",
    " '—à–ª—é—Ö–∞',\n",
    " '–æ—á–∫–æ',\n",
    " '—Ñ–∞—à–∏—Å—Ç',\n",
    " '–µ–±–µ—Ç–∞',\n",
    " '–æ—Ç—Ä—É–±–∏—Ç—å',\n",
    " '–ø–æ—Å–æ—Å–∞—Ç—å',\n",
    " '—Å–¥–æ—Ö–Ω—É—Ç—å',\n",
    " '—Å—Ç—Ä–µ–ª—è—Ç—å',\n",
    " '—É–Ω–∏—á—Ç–æ–∂–∞—Ç—å',\n",
    " '—Ä–∞–∫',\n",
    " '—Å–∂–µ—á—å',\n",
    " '–≥–∞–¥',\n",
    " '—Å–∏—Å—å–∫–∞',\n",
    " '—ë–±–∞–Ω—ã–π',\n",
    " '–ø—Ä–∏–±–∏—Ç—å',\n",
    " '—Ç—Ä–∞—Ö–∞—Ç',\n",
    " '–¥–µ–±–∏–ª',\n",
    " '—à–∞–ª–∞–≤—ã–π',\n",
    " '—Ä–∞—Å—Ç—Ä–µ–ª–∏–≤–∞—Ç—å',\n",
    " '–Ω–µ–≥—Ä',\n",
    " '–æ—Ç—Å–∞—Å—ã–≤–∞—Ç—å',\n",
    " '–æ—Ç–æ—Ä–≤–∞—Ç—å',\n",
    " '—Ö–µ—Ä',\n",
    " '—É—Ä–æ–¥',\n",
    " '–ø–∏–¥–∞—Ä–∞—Å',\n",
    " '–∫–∞—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å',\n",
    " '–ø–æ–ø',\n",
    " '–ø—Ä–∏—Å—Ç—Ä–µ–ª–∏—Ç—å',\n",
    " '–ø–∞–¥–ª–æ',\n",
    " '—Ç—Ä–∞—Ö–Ω—É—Ç—å',\n",
    " '–∑–∞—Å—É–Ω—É—Ç—å',\n",
    " '–º–æ—Ä–¥–∞',\n",
    " '–≤—ã–µ–±–∞—Ç—å',\n",
    " '–∂–æ–ø–∞',\n",
    " '—Å—Ä–∞–∫',\n",
    " '–∂–∏–≤—å—ë–º',\n",
    " '—Ö—É–π–Ω—è',\n",
    " '–º–æ—á–∏—Ç—å',\n",
    " '—Ä–∞—Å—Å—Ç—Ä–µ–ª—è—Ç—å',\n",
    " '–∞–¥',\n",
    " '—Ä–∞—Å—Å—Ç—Ä–µ–ª',\n",
    " '–µ–±—É—Ç',\n",
    " '—É–Ω–∏—á—Ç–æ–∂–∏—Ç—å',\n",
    " '–ø–æ–¥–≤–µ—Å–∏—Ç—å',\n",
    " '–≥–∞–Ω–¥–æ–Ω',\n",
    " '–º—Ä–∞–∑—å',\n",
    " '–ø–æ–¥—Ä–æ—á–∏—Ç—å',\n",
    " '—Ä–∞—Å—Å—Ç—Ä–µ–ª–∏–≤–∞—Ç—å',\n",
    " '–±–ª—è',\n",
    " '–æ—Ç–ª–∏–∑–∞—Ç—å',\n",
    " '–µ–±–∞–Ω—É—Ç—å',\n",
    " '–≤–µ—à–∞—Ç—å',\n",
    " '—Ö—É–π —Å–æ—Å–∞—Ç—å',\n",
    " '–≥–æ—Ä–µ—Ç—å',\n",
    " '–∫–æ–ª',\n",
    " '–ø–∏–¥–æ—Ä',\n",
    " '—Å–≤–æ–ª–æ—á—å',\n",
    " '–∏–¥–∏–æ—Ç',\n",
    " '–µ–±–∞—Ç—å',\n",
    " '–ø–µ—Ç—É—Ö',\n",
    " '–ø–∏—Å—è',\n",
    " '—Ä–æ—Ç–∏–∫',\n",
    " '–ø–æ–ø–∫–∞',\n",
    " '—Ö—Ä–µ–Ω',\n",
    " '—Å—É—á–∫–∞',\n",
    " '–Ω–∞–∫–∞–∑–∞—Ç—å',\n",
    " '—Ö—É–π',\n",
    " '–¥–æ–ª–±–æ–µ–±',\n",
    " '–¥—ã—Ä–∫–∞',\n",
    " '—Å–æ—Å–∞—Ç—å —Ö—É–π',\n",
    " '—Ç–æ—á–Ω–æ',\n",
    " '–Ω–∞—Ö—É–π',\n",
    " '—Ç—É–ø–æ–π',\n",
    " '–≤–æ–Ω—é—á–∏–π',\n",
    " '—Ç—Ä–∞—Ö–∞—Ç—å—Å—è',\n",
    " '—Ç—Ä–∞—Ö–∞—Ç—å',\n",
    " '—Ç–≤–∞—Ä—å',\n",
    " '—Ä–æ–∂–∞',\n",
    " '—á–ª–µ–Ω',\n",
    " '–≥–æ—Ä–µ—Ç—å –∞–¥',\n",
    " '–æ—Ç—Å–æ—Å–∞—Ç—å',\n",
    " '–¥–æ–ª–±–æ–µ—Å—Ç–∏',\n",
    " '—Å–æ—Å–∞—Ç—å',\n",
    " '–ø–∏–¥–∞—Ä',\n",
    " '—É–±–∏—Ç—å',\n",
    " '—Å–º–µ—Ä—Ç—å']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GTRux1Go0OMg"
   },
   "outputs": [],
   "source": [
    "def hyperopt_tdidf_logit_label(label):\n",
    "    \n",
    "    X_u = XY_train.text\n",
    "    X_l = XY_train.text_lemmatised\n",
    "    y = XY_train[label]\n",
    "    \n",
    "    @ignore_warnings(category=ConvergenceWarning)\n",
    "    def hyperopt_tdidf_logit(params):\n",
    "        \n",
    "        X_1 = TfidfVectorizer(min_df = int(params['min_df']), ngram_range = (1, 1)).fit_transform(X_u)\n",
    "        X_2 = CountVectorizer(vocabulary = vocab, binary = True).fit_transform(X_l)\n",
    "        \n",
    "        X = hstack([X_1, X_2])\n",
    "        \n",
    "        score = cross_val_score(estimator = LogisticRegression(C = params['C']), X = X, y = y,\n",
    "                                cv = StratifiedKFold(n_splits = 7), scoring = 'average_precision')\n",
    "\n",
    "        score_mean = score.mean()\n",
    "        \n",
    "        return -score_mean\n",
    "\n",
    "    space_tfidf_logit = {'C': hp.uniform('C', 0.1, 25),\n",
    "                         'min_df': hp.quniform('min_df', 2, 12, 1)}\n",
    "    \n",
    "    best = fmin(fn = hyperopt_tdidf_logit, space = space_tfidf_logit, algo = tpe.suggest, max_evals = 75)\n",
    "    \n",
    "    tfidf = TfidfVectorizer(min_df = int(best['min_df']), ngram_range = (1, 1))\n",
    "    count = CountVectorizer(vocabulary = vocab, binary = True)\n",
    "    \n",
    "    X_1 = tfidf.fit_transform(X_u)\n",
    "    X_2 = count.fit_transform(X_l)\n",
    "    \n",
    "    X = hstack([X_1, X_2])\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    \n",
    "    ls = LogisticRegression(C = best['C'])\n",
    "    \n",
    "    ls.fit(X, y)\n",
    "    \n",
    "    print(average_precision_score(y_score = ls.predict_proba(X)[:, 1], y_true = y))\n",
    "    \n",
    "    return {'tf': tfidf, 'coun': count, 'logit': ls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OlRwm1Vh0OMm",
    "outputId": "ef24e8fe-53db-46c3-84ec-c97061d710d4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "normal\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [11:29<00:00, 13.78s/trial, best loss: -0.9939286414522565]\n",
      "0.9991902153444302\n",
      "----------------------------------------------------------------------------------------------------\n",
      "insult\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [11:59<00:00, 14.38s/trial, best loss: -0.9110842515877565]\n",
      "0.9896949743792532\n",
      "----------------------------------------------------------------------------------------------------\n",
      "obscenity\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [13:00<00:00, 15.61s/trial, best loss: -0.8042385337524033]\n",
      "0.9889213308636742\n",
      "----------------------------------------------------------------------------------------------------\n",
      "threat\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [12:35<00:00, 15.10s/trial, best loss: -0.8877666029333326]\n",
      "0.9808049840012112\n"
     ]
    }
   ],
   "source": [
    "algs_1 = dict()\n",
    "for label in ['normal', 'insult', 'obscenity', 'threat']:\n",
    "    print('----------------------------------------------------------------------------------------------------')\n",
    "    print(label)\n",
    "    print('----------------------------------------------------------------------------------------------------')\n",
    "    print()\n",
    "    algs_1[label] = hyperopt_tdidf_logit_label(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJRsdRRMOQnZ",
    "outputId": "04af64bf-64b6-4feb-c2ad-1db27fae4e36",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'normal': {'tf': TfidfVectorizer(min_df=2),\n",
       "  'coun': CountVectorizer(binary=True,\n",
       "                  vocabulary=['–±—ã–¥–ª–æ', '–¥—Ä–æ—á–∏—Ç—å', '–∫–æ–Ω—á–∏—Ç—å', '–æ—Ç—Ä–µ–∑–∞—Ç—å', '–µ–±–∞–ª',\n",
       "                              '–ø–∏–∑–¥–µ—Ü', '–≤–¥—É—Ç—å', '–ø–æ–ª–∏–∑–∞—Ç—å', '—Å–æ—Å–Ω—É—Ç—å',\n",
       "                              '–ø–æ–≤–µ—Å–∏—Ç—å', '—É—Ç–æ–ø–∏—Ç—å', '–ø–∏–∑–¥–∞', '—Å–ø–µ—Ä–º–∞', '–∫–æ–∑—ë–ª',\n",
       "                              '–ø–∏–¥–æ—Ä–∞—Å', '–Ω–∞–±–∏—Ç—å', '–≥–æ–≤–Ω–æ', '–∑–∞–∫–æ–ø–∞—Ç—å', '—Ö–æ—Ö–æ–ª',\n",
       "                              '–≤–æ—Ä', '–¥—É—Ä–∞', '—É–±–ª—é–¥–æ–∫', '–∫–∞–∑–Ω–∏—Ç—å', '–≤—ã–µ–±–∞–ª',\n",
       "                              '–ª–∏–∑–∞—Ç—å', '–∑–∞—Å–∞–¥–∏—Ç—å', '—á–º–æ', '—Ä–∞—Å—Ç—Ä–µ–ª–∞', '–∑–∞–¥–Ω–∏—Ü–∞',\n",
       "                              '–ø—Ä–∏–ª—é–¥–Ω–æ', ...]),\n",
       "  'logit': LogisticRegression(C=5.123683620913078)},\n",
       " 'insult': {'tf': TfidfVectorizer(min_df=2),\n",
       "  'coun': CountVectorizer(binary=True,\n",
       "                  vocabulary=['–±—ã–¥–ª–æ', '–¥—Ä–æ—á–∏—Ç—å', '–∫–æ–Ω—á–∏—Ç—å', '–æ—Ç—Ä–µ–∑–∞—Ç—å', '–µ–±–∞–ª',\n",
       "                              '–ø–∏–∑–¥–µ—Ü', '–≤–¥—É—Ç—å', '–ø–æ–ª–∏–∑–∞—Ç—å', '—Å–æ—Å–Ω—É—Ç—å',\n",
       "                              '–ø–æ–≤–µ—Å–∏—Ç—å', '—É—Ç–æ–ø–∏—Ç—å', '–ø–∏–∑–¥–∞', '—Å–ø–µ—Ä–º–∞', '–∫–æ–∑—ë–ª',\n",
       "                              '–ø–∏–¥–æ—Ä–∞—Å', '–Ω–∞–±–∏—Ç—å', '–≥–æ–≤–Ω–æ', '–∑–∞–∫–æ–ø–∞—Ç—å', '—Ö–æ—Ö–æ–ª',\n",
       "                              '–≤–æ—Ä', '–¥—É—Ä–∞', '—É–±–ª—é–¥–æ–∫', '–∫–∞–∑–Ω–∏—Ç—å', '–≤—ã–µ–±–∞–ª',\n",
       "                              '–ª–∏–∑–∞—Ç—å', '–∑–∞—Å–∞–¥–∏—Ç—å', '—á–º–æ', '—Ä–∞—Å—Ç—Ä–µ–ª–∞', '–∑–∞–¥–Ω–∏—Ü–∞',\n",
       "                              '–ø—Ä–∏–ª—é–¥–Ω–æ', ...]),\n",
       "  'logit': LogisticRegression(C=18.193567836867494)},\n",
       " 'obscenity': {'tf': TfidfVectorizer(min_df=2),\n",
       "  'coun': CountVectorizer(binary=True,\n",
       "                  vocabulary=['–±—ã–¥–ª–æ', '–¥—Ä–æ—á–∏—Ç—å', '–∫–æ–Ω—á–∏—Ç—å', '–æ—Ç—Ä–µ–∑–∞—Ç—å', '–µ–±–∞–ª',\n",
       "                              '–ø–∏–∑–¥–µ—Ü', '–≤–¥—É—Ç—å', '–ø–æ–ª–∏–∑–∞—Ç—å', '—Å–æ—Å–Ω—É—Ç—å',\n",
       "                              '–ø–æ–≤–µ—Å–∏—Ç—å', '—É—Ç–æ–ø–∏—Ç—å', '–ø–∏–∑–¥–∞', '—Å–ø–µ—Ä–º–∞', '–∫–æ–∑—ë–ª',\n",
       "                              '–ø–∏–¥–æ—Ä–∞—Å', '–Ω–∞–±–∏—Ç—å', '–≥–æ–≤–Ω–æ', '–∑–∞–∫–æ–ø–∞—Ç—å', '—Ö–æ—Ö–æ–ª',\n",
       "                              '–≤–æ—Ä', '–¥—É—Ä–∞', '—É–±–ª—é–¥–æ–∫', '–∫–∞–∑–Ω–∏—Ç—å', '–≤—ã–µ–±–∞–ª',\n",
       "                              '–ª–∏–∑–∞—Ç—å', '–∑–∞—Å–∞–¥–∏—Ç—å', '—á–º–æ', '—Ä–∞—Å—Ç—Ä–µ–ª–∞', '–∑–∞–¥–Ω–∏—Ü–∞',\n",
       "                              '–ø—Ä–∏–ª—é–¥–Ω–æ', ...]),\n",
       "  'logit': LogisticRegression(C=13.816818739210584)},\n",
       " 'threat': {'tf': TfidfVectorizer(min_df=3),\n",
       "  'coun': CountVectorizer(binary=True,\n",
       "                  vocabulary=['–±—ã–¥–ª–æ', '–¥—Ä–æ—á–∏—Ç—å', '–∫–æ–Ω—á–∏—Ç—å', '–æ—Ç—Ä–µ–∑–∞—Ç—å', '–µ–±–∞–ª',\n",
       "                              '–ø–∏–∑–¥–µ—Ü', '–≤–¥—É—Ç—å', '–ø–æ–ª–∏–∑–∞—Ç—å', '—Å–æ—Å–Ω—É—Ç—å',\n",
       "                              '–ø–æ–≤–µ—Å–∏—Ç—å', '—É—Ç–æ–ø–∏—Ç—å', '–ø–∏–∑–¥–∞', '—Å–ø–µ—Ä–º–∞', '–∫–æ–∑—ë–ª',\n",
       "                              '–ø–∏–¥–æ—Ä–∞—Å', '–Ω–∞–±–∏—Ç—å', '–≥–æ–≤–Ω–æ', '–∑–∞–∫–æ–ø–∞—Ç—å', '—Ö–æ—Ö–æ–ª',\n",
       "                              '–≤–æ—Ä', '–¥—É—Ä–∞', '—É–±–ª—é–¥–æ–∫', '–∫–∞–∑–Ω–∏—Ç—å', '–≤—ã–µ–±–∞–ª',\n",
       "                              '–ª–∏–∑–∞—Ç—å', '–∑–∞—Å–∞–¥–∏—Ç—å', '—á–º–æ', '—Ä–∞—Å—Ç—Ä–µ–ª–∞', '–∑–∞–¥–Ω–∏—Ü–∞',\n",
       "                              '–ø—Ä–∏–ª—é–¥–Ω–æ', ...]),\n",
       "  'logit': LogisticRegression(C=7.625769305012529)}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algs_1\n",
    "# 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [32:12<00:00, 19.33s/trial, best loss: -0.9052236649167692] - insult\n",
    "\n",
    "# 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [36:35<00:00, 21.95s/trial, best loss: -0.8018728019201194] - obscenity\n",
    "\n",
    "# 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [34:22<00:00, 20.63s/trial, best loss: -0.8884257267154007] - threat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UAexmbXV0ONc",
    "outputId": "8617ec54-4bec-4d7a-ddd2-589e23a22347",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_lemmatised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167315</td>\n",
       "      <td>–∫–∞–∫–∞—è –ø—Ä–µ–ª–µ—Å—Ç—å!!!üòç</td>\n",
       "      <td>–∫–∞–∫–∞—è –ø—Ä–µ–ª–µ—Å—Ç—åüòç</td>\n",
       "      <td>–∫–∞–∫–æ–π –ø—Ä–µ–ª–µ—Å—Ç—åüòç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>224546</td>\n",
       "      <td>–∫–∞–∞–ª –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤—å—é?</td>\n",
       "      <td>–∫–∞–∞ –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤</td>\n",
       "      <td>–∫–∞–∞–ª–∞ –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤—å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>241309</td>\n",
       "      <td>–≥–Ω–æ–π–Ω—ã–µ –ø–∏–¥–æ—Ä—ã –∞–ª–ª—ã –æ–Ω–∏</td>\n",
       "      <td>–≥–Ω–æ–π–Ω –ø–∏–¥–æ—Ä –∞–ª–ª –æ–Ω–∏</td>\n",
       "      <td>–≥–Ω–æ–π–Ω—ã–π –ø–∏–¥–æ—Ä –∞–ª–ª–∞ –æ–Ω–∏</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                     text         text_stemmed  \\\n",
       "0  167315       –∫–∞–∫–∞—è –ø—Ä–µ–ª–µ—Å—Ç—å!!!üòç      –∫–∞–∫–∞—è –ø—Ä–µ–ª–µ—Å—Ç—åüòç   \n",
       "1  224546  –∫–∞–∞–ª –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤—å—é?  –∫–∞–∞ –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤   \n",
       "2  241309  –≥–Ω–æ–π–Ω—ã–µ –ø–∏–¥–æ—Ä—ã –∞–ª–ª—ã –æ–Ω–∏  –≥–Ω–æ–π–Ω –ø–∏–¥–æ—Ä –∞–ª–ª –æ–Ω–∏   \n",
       "\n",
       "          text_lemmatised  \n",
       "0         –∫–∞–∫–æ–π –ø—Ä–µ–ª–µ—Å—Ç—åüòç  \n",
       "1  –∫–∞–∞–ª–∞ –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤—å  \n",
       "2  –≥–Ω–æ–π–Ω—ã–π –ø–∏–¥–æ—Ä –∞–ª–ª–∞ –æ–Ω–∏  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final_test = pd.read_csv('X_final_test.csv', header = 0)\n",
    "X_final_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HuiwWJcg0ONb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# trans = TfidfVectorizer(min_df = 2)\n",
    "# text_train = trans.fit_transform(XY.text)\n",
    "# text_test = trans.transform(X_final_test.text)\n",
    "\n",
    "\n",
    "# clf_normal = LogisticRegression(C = 6.4).fit(text_train, XY['normal'])\n",
    "# clf_insult = LogisticRegression(C = 11.2).fit(text_train, XY['insult'])\n",
    "# clf_threat = LogisticRegression(C = 10.6).fit(text_train, XY['threat'])\n",
    "\n",
    "# pred_test_normal = clf_normal.predict_proba(text_test)[:, 1].reshape(-1, 1)\n",
    "# pred_test_insult = clf_insult.predict_proba(text_test)[:, 1].reshape(-1, 1)\n",
    "# pred_test_threat = clf_threat.predict_proba(text_test)[:, 1].reshape(-1, 1)\n",
    "\n",
    "\n",
    "trans_1_normal = TfidfVectorizer(min_df = 2, ngram_range = (1, 1))\n",
    "text_1_train_normal = trans_1_normal.fit_transform(XY.text)\n",
    "text_1_test_normal = trans_1_normal.transform(X_final_test.text)\n",
    "\n",
    "trans_1_insult = TfidfVectorizer(min_df = 2, ngram_range = (1, 1))\n",
    "text_1_train_insult = trans_1_insult.fit_transform(XY.text)\n",
    "text_1_test_insult = trans_1_insult.transform(X_final_test.text)\n",
    "\n",
    "trans_1_threat = TfidfVectorizer(min_df = 3, ngram_range = (1, 1))\n",
    "text_1_train_threat = trans_1_threat.fit_transform(XY.text)\n",
    "text_1_test_threat = trans_1_threat.transform(X_final_test.text)\n",
    "\n",
    "trans_1_obscenity = TfidfVectorizer(min_df = 2, ngram_range = (1, 1))\n",
    "text_1_train_obscenity = trans_1_obscenity.fit_transform(XY.text)\n",
    "text_1_test_obscenity = trans_1_obscenity.transform(X_final_test.text)\n",
    "\n",
    "\n",
    "trans_2 = CountVectorizer(vocabulary = vocab, binary = True)\n",
    "text_2_train = trans_2.fit_transform(XY.text_lemmatised)\n",
    "text_2_test = trans_2.transform(X_final_test.text_lemmatised)\n",
    "\n",
    "\n",
    "text_train_normal = hstack([text_1_train_normal, text_2_train])\n",
    "text_train_insult = hstack([text_1_train_insult, text_2_train])\n",
    "text_train_threat = hstack([text_1_train_threat, text_2_train])\n",
    "text_train_obscenity = hstack([text_1_train_obscenity, text_2_train])\n",
    "\n",
    "text_test_normal = hstack([text_1_test_normal, text_2_test])\n",
    "text_test_insult = hstack([text_1_test_insult, text_2_test])\n",
    "text_test_threat = hstack([text_1_test_threat, text_2_test])\n",
    "text_test_obscenity = hstack([text_1_test_obscenity, text_2_test])\n",
    "\n",
    "\n",
    "clf_normal = LogisticRegression(C = 5.12).fit(text_train_normal, XY['normal'])\n",
    "clf_insult = LogisticRegression(C = 18.19).fit(text_train_insult, XY['insult'])\n",
    "clf_threat = LogisticRegression(C = 7.63).fit(text_train_threat, XY['threat']) # 19.775484\n",
    "clf_obscenity = LogisticRegression(C = 13.82).fit(text_train_obscenity, XY['obscenity']) # 20.8466131\n",
    "\n",
    "pred_test_normal = clf_normal.predict_proba(text_test_normal)[:, 1].reshape(-1, 1)\n",
    "pred_test_insult = clf_insult.predict_proba(text_test_insult)[:, 1].reshape(-1, 1)\n",
    "pred_test_threat = clf_threat.predict_proba(text_test_threat)[:, 1].reshape(-1, 1)\n",
    "pred_test_obscenity = clf_obscenity.predict_proba(text_test_obscenity)[:, 1].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TWbe9_sZ0ONi"
   },
   "outputs": [],
   "source": [
    "predictions = np.hstack([pred_test_normal, pred_test_insult, pred_test_threat, pred_test_obscenity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['normal', 'insult', 'threat', 'obscenity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "dpk6iVIX0ONj"
   },
   "outputs": [],
   "source": [
    "final_predictions = pd.concat([pd.DataFrame(X_final_test.id.values, columns = ['id']),\n",
    "                               pd.DataFrame(predictions, columns = labels)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UbahSrcW0ONk"
   },
   "outputs": [],
   "source": [
    "result = final_predictions.loc[:, ['id', 'normal', 'insult', 'obscenity', 'threat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>normal</th>\n",
       "      <th>insult</th>\n",
       "      <th>obscenity</th>\n",
       "      <th>threat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167315</td>\n",
       "      <td>0.998261</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>224546</td>\n",
       "      <td>0.946274</td>\n",
       "      <td>0.033348</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>0.030082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>241309</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>0.019074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31170</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.999731</td>\n",
       "      <td>0.014532</td>\n",
       "      <td>0.015765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173358</td>\n",
       "      <td>0.947059</td>\n",
       "      <td>0.061836</td>\n",
       "      <td>0.004384</td>\n",
       "      <td>0.001584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    normal    insult  obscenity    threat\n",
       "0  167315  0.998261  0.000232   0.000150  0.000080\n",
       "1  224546  0.946274  0.033348   0.002244  0.030082\n",
       "2  241309  0.003021  0.999378   0.001123  0.019074\n",
       "3   31170  0.000455  0.999731   0.014532  0.015765\n",
       "4  173358  0.947059  0.061836   0.004384  0.001584"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "jfiODFw10ONm"
   },
   "outputs": [],
   "source": [
    "result.to_csv('result', index = False, header = True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_6 (simple hyperopt).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
