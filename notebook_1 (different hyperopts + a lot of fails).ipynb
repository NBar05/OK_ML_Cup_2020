{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.txt') as f:\n",
    "    data = [s.replace('__label__', '').split('\\t') for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 143787, 2: 4877, 3: 111})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(np.array([len(s)-2 for s in data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "[labels.extend(s[1:-1]) for s in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'THREAT': 7164, 'NORMAL': 122194, 'INSULT': 21952, 'OBSCENITY': 2564})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XY = pd.DataFrame(columns = ['id', 'text', 'exclamation_num', 'question_num',\n",
    "#                              'normal', 'threat', 'insult', 'obscenity'])\n",
    "# XY['id'] = [s[0] for s in data]\n",
    "# XY['text'] = [s[-1].strip() for s in data]\n",
    "# XY['text_stemmed'] = stem_words(XY['text'])\n",
    "# XY['text_lemmatised'] = lemmatise_words(XY['text']) # 9min 18s\n",
    "# XY['exclamation_num'] = XY.text.str.count('!')\n",
    "# XY['question_num'] = XY.text.str.count('\\?')\n",
    "# XY['stars_num'] = XY.text.str.count('.')\n",
    "\n",
    "# statuses = pd.DataFrame([s[1:-1] for s in data])\n",
    "# XY.normal = np.sum(np.where(statuses.values == 'NORMAL', 1, 0), axis = 1)\n",
    "# XY.threat = np.sum(np.where(statuses.values == 'THREAT', 1, 0), axis = 1)\n",
    "# XY.insult = np.sum(np.where(statuses.values == 'INSULT', 1, 0), axis = 1)\n",
    "# XY.obscenity = np.sum(np.where(statuses.values == 'OBSCENITY', 1, 0), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "threat       0\n",
       "insult       0\n",
       "obscenity    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY.loc[XY.normal == 1, 'threat':'obscenity'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 2, figsize = (15, 4))\n",
    "# sns.distplot(XY.loc[(XY.threat == 1)].exclamation_num, \n",
    "#              kde = False, color = 'r', ax = ax[0])\n",
    "# sns.distplot(XY.loc[(XY.threat == 0)].exclamation_num, \n",
    "#              kde = False, color = 'b', ax = ax[1])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.871998883305416, 0.6199306551044763)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY.loc[(XY.threat == 1)].exclamation_num.mean(), XY.loc[(XY.threat == 0)].exclamation_num.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 2, figsize = (15, 4))\n",
    "# sns.distplot(XY.loc[(XY.threat == 1)].question_num, \n",
    "#              kde = False, color = 'r', ax = ax[0])\n",
    "# sns.distplot(XY.loc[(XY.threat == 0)].question_num, \n",
    "#              kde = False, color = 'b', ax = ax[1])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08458961474036851, 0.23243250877403593)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY.loc[(XY.threat == 1)].question_num.mean(), XY.loc[(XY.threat == 0)].question_num.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train, XY_test = train_test_split(XY, test_size = 0.3, shuffle = True, random_state = 42)\n",
    "XY_train.reset_index(drop = True, inplace = True)\n",
    "XY_test.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal       0.001908\n",
       "threat      -0.000569\n",
       "insult      -0.002059\n",
       "obscenity   -0.000665\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY_train.loc[:, 'normal':'obscenity'].mean(axis = 0) - XY_test.loc[:, 'normal':'obscenity'].mean(axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¥–µ–±–∏–ª, —Å–∫—Ä—ã–≤–∞—Ç—å –Ω–µ—á–µ–≥–æ —ç—Ç–æ —Ç–æ–ª—å–∫–æ –≤ –≤–∞—à–∏—Ö —Ñ–∞–Ω—Ç–∞–∑–∏—è—Ö!\n",
      "\n",
      "–≥–æ—Å–ø–æ–¥–∏,—Ç–∞–∫–∞—è –º–æ–ª–æ–¥–∞—è!!!\n",
      "\n",
      "–≤—Å–µ–º –¥–æ–±—Ä—ã–π –¥–µ–Ω—å.–ø—Ä–∏–≥–ª–∞—à–∞—é –∞–∑–∞—Ä—Ç–Ω—ã—Ö –∂–µ–ª–∞—é—â–∏—Ö –∏–º–µ–Ω–Ω–æ –ø–æ–±–µ–∂–¥–∞—Ç—å –∏–≥—Ä–æ–∫–æ–≤ –≤ –∑–∞–±–∞–≤—ã.—Å–µ–π—á–∞—Å –ª–∏–≥–∞ —Ä–æ–∑–æ–≤–∞—è —É—Å–∫–æ—Ä—è–µ–º –ª—é–±–∏–º –ø–æ–±–µ–∂–¥–∞—Ç—å.–∑–∞–¥–∞–Ω–∏—è 320/400 +–¥–æ–ø –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã –ø–∏—à–∏—Ç–µ –≤ –ª–∏—á–∫—É.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in XY_train.loc[XY_train.normal == 1].iloc[[10, 100, 1000]].iterrows():\n",
    "    print(row.text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—Å–¥–æ—Ö–Ω–∏ —Ç–≤–∞—Ä—å).—ç—Ç–æ –∫–∞–∫–æ–µ —Å–µ—Ä–¥—Ü–µ –Ω—É–∂–Ω–æ –∏–º–µ—Ç—å, —á—Ç–æ–±—ã —Å–∏–æ—Ç—Ä–µ—Ç—å –≤ –≥–ª–∞–∑–∞ —ç—Ç–æ–º—É –ø–µ—Å–∏–∫—É –Ω–µ—Å—á–∞—Å—Ç–Ω–æ–º—É.—è –ø—Ä–æ—Å—Ç–æ –ø–ª–∞—á—É.:(\n",
      "\n",
      "—É–±–ª—é–¥–∫–∏, –¥–æ–ª–±–∞–Ω—É—Ç—ã–µ, –ø—Ä–∏–≤—è–∑–∞—Ç—å –∑–∞ —è–π—Ü–∞ –∏ —Ç–∞–∫ –∂–µ –ø—Ä–æ—Ç–∞—â–∏—Ç—å.\n",
      "\n",
      "–≤—Å–µ—Ö —ç—Ç–∏—Ö —Å—É–∫ –∫ —Å—Ç–µ–Ω–∫–µ, –≤—Å–µ—Ö, –≤—Å–µ—Ö, –≤—Å–µ—Ö\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in XY_train.loc[XY_train.threat == 1].iloc[[10, 100, 1000]].iterrows():\n",
    "    print(row.text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–Ω–∞ –∫—É—Å–æ—á–∫–∏ —ç—Ç–æ–≥–æ –∫–æ–∑–ª–∞ —Ä–∞–∑–æ—Ä–≤–∞—Ç—å.\n",
      "\n",
      "–¥–∞–≤–Ω–æ –ø–æ—Ä–∞, —É–µ–±–æ–∫ –¥–æ—Å—Ç–∞–ª. ü§¨ü§¨ü§¨\n",
      "\n",
      "–∞ —É–Ω–∞—Å —Å–º–∏ –≤—Å–µ –ø–∏–∑–¥—É–Ω—ã –≤—Å—ë –≤–æ–∫—Ä—É–≥ –æ–¥–Ω–æ –≤—Ä–∞–Ω—å—ë –∂—É–ª—å—ë –µ–≤—Ä–µ–π—Å–∫–æ–µ —Ç–∞–∫ —á—Ç–æ –ª—É—á—à–µ –º—ã –∂–∏—Ç—å –Ω–µ –±—É–¥–µ–º\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in XY_train.loc[XY_train.insult == 1].iloc[[10, 100, 1000]].iterrows():\n",
    "    print(row.text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—É –∞–Ω–∞–ª—å–Ω–æ–π —Å—É—á–∫–∏ –ø–æ—Ö–æ–¥—É —Å–µ–¥–Ω—è –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ –∫—Ä—ã—à—É —Å–Ω–µ—Å–ª–æ#u62b889dffds#—Ç—ã —Ç–æ–∫–∞ –≥–ª—è–Ω—å –∫–∞–∫—É—é –ø—É—Ä–≥—É –æ–Ω–∞ –Ω–µ—Å–µ—Ç#u622dfc4586s# #u888b947ff8s#\n",
      "\n",
      "—è—Ç–≤–æ—é–º–∞–º—É –µ–±–∞–ª –∫–æ–≥–¥–∞ –æ–Ω–∞ –µ—â–µ –±—ã–ª–∞ –≤–Ω—É—Ç—Ä–∏ –±–∞–±—É—à–∫–∏ –≥–∏s–æv–∞—Å–Ω–≥–∏\n",
      "\n",
      "—Ö—É–π–ª–æ –¥—ã—Ä—è–≤–æ–µ, –≥—É–±—ã –≤—ã—Ç—è–Ω—É–ª, –æ—Ç—Å–æ—Å–∞—Ç—å —Ö–æ—è–µ—Ç.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in XY_train.loc[XY_train.obscenity == 1].iloc[[10, 100, 1000]].iterrows():\n",
    "    print(row.text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (15, 3))\n",
    "sns.barplot(data = XY_train.loc[:, 'threat':'obscenity'].sum().reset_index().rename(columns = \\\n",
    "            {0: 'count', 'index': 'labels'}), x = 'labels', y = 'count', ax = ax[0])\n",
    "sns.barplot(data = XY_test.loc[:, 'threat':'obscenity'].sum().reset_index().rename(columns = \\\n",
    "            {0: 'count', 'index': 'labels'}), x = 'labels', y = 'count', ax = ax[1])\n",
    "ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation = 0)\n",
    "ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, figsize = (12, 5))\n",
    "# crosses = dict(Counter(XY_abn.threat.astype('str') + XY_abn.insult.astype('str') + XY_abn.obscenity.astype('str')))\n",
    "# sns.barplot(x = list(crosses.keys()), y = list(crosses.values()))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XY_abn.groupby('threat').agg({'insult': np.mean, 'obscenity': np.mean})\n",
    "# XY_abn.groupby('insult').agg({'threat': np.mean, 'obscenity': np.mean})\n",
    "# XY_abn.groupby('obscenity').agg({'threat': np.mean, 'insult': np.mean})\n",
    "\n",
    "# XY_abn.groupby(['insult', 'threat']).agg({'obscenity': np.mean})\n",
    "# XY_abn.groupby(['insult', 'obscenity']).agg({'threat': np.mean})\n",
    "# XY_abn.groupby(['threat', 'obscenity']).agg({'insult': np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY_train['text'].str.findall(r'[A-Z]').str.len().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label predictions\n",
    "## Function manufacture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(text, stemmer = SnowballStemmer(\"russian\", ignore_stopwords = True)):\n",
    "    clean_sents = [s.translate(str.maketrans('', '', string.punctuation)) for s in text.tolist()]\n",
    "    list_of_sent_lists = [sent.split(' ') for sent in clean_sents]\n",
    "    \n",
    "    new_stemmed_sentences = []\n",
    "    for sent_list in list_of_sent_lists:\n",
    "        new_stemmed_sentences.append(' '.join([stemmer.stem(word) for word in sent_list]))\n",
    "    \n",
    "    return new_stemmed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-beb1e0106e9c>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  XY_train['text_stemmed'] = stem_words(XY_train['text'])\n",
      "<ipython-input-27-beb1e0106e9c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  XY_test['text_stemmed'] = stem_words(XY_test['text'])\n"
     ]
    }
   ],
   "source": [
    "XY_train['text_stemmed'] = stem_words(XY_train['text'])\n",
    "XY_test['text_stemmed'] = stem_words(XY_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–º–æ–∂–Ω–æ —Ç–µ–ª</td>\n",
       "      <td>–º–æ–∂–Ω–æ —Ç–µ–ª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–Ω–∞–¥–µ–π—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–µ–±—è,—É–ø–∞–ª –≤—Å—Ç–∞–≤–∞–π –∏ –¥–∞–ª—å—à–µ –∏...</td>\n",
       "      <td>–Ω–∞–¥ —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–µ–±—è—É–ø–∞ –≤—Å—Ç–∞–≤–∞ –∏ –¥–∞–ª—å—à –∏–¥–∏–Ω–∏–∫—Ç —Ç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–º–æ–∂–µ—Ç –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞–¥–æ –±—ã–ª–æ</td>\n",
       "      <td>–º–æ–∂–µ—Ç –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑ –Ω–∞–¥–æ –±—ã–ª–æ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ —á—Ç–æ –æ–Ω–∏ –∫—É—Ä—è—Ç –ø–µ—Ä–µ–¥ –∑–∞—Å–µ–¥–∞–Ω–∏—è–º–∏(h)</td>\n",
       "      <td>–∏–Ω—Ç–µ—Ä–µ—Å–Ω —á—Ç–æ –æ–Ω–∏ –∫—É—Ä –ø–µ—Ä–µ–¥ –∑–∞—Å–µ–¥–∞–Ω–∏—è–º–∏h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–Ω–µ –∑–Ω–∞—é –≥–¥–µ –≤–∞—Å –Ω–∞ –¥—Ä—É–∂–±–µ,–∞ —É –Ω–∞—Å –Ω–∞ –ª–æ–º–æ–Ω–æ—Å–æ–≤...</td>\n",
       "      <td>–Ω–µ –∑–Ω–∞ –≥–¥–µ –≤–∞—Å –Ω–∞ –¥—Ä—É–∂–±–µ —É –Ω–∞—Å –Ω–∞ –ª–æ–º–æ–Ω–æ—Å–æ–≤ —Å–æ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                          –º–æ–∂–Ω–æ —Ç–µ–ª   \n",
       "1  –Ω–∞–¥–µ–π—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–µ–±—è,—É–ø–∞–ª –≤—Å—Ç–∞–≤–∞–π –∏ –¥–∞–ª—å—à–µ –∏...   \n",
       "2                      –º–æ–∂–µ—Ç –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞–¥–æ –±—ã–ª–æ   \n",
       "3       –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ —á—Ç–æ –æ–Ω–∏ –∫—É—Ä—è—Ç –ø–µ—Ä–µ–¥ –∑–∞—Å–µ–¥–∞–Ω–∏—è–º–∏(h)   \n",
       "4  –Ω–µ –∑–Ω–∞—é –≥–¥–µ –≤–∞—Å –Ω–∞ –¥—Ä—É–∂–±–µ,–∞ —É –Ω–∞—Å –Ω–∞ –ª–æ–º–æ–Ω–æ—Å–æ–≤...   \n",
       "\n",
       "                                        text_stemmed  \n",
       "0                                          –º–æ–∂–Ω–æ —Ç–µ–ª  \n",
       "1  –Ω–∞–¥ —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–µ–±—è—É–ø–∞ –≤—Å—Ç–∞–≤–∞ –∏ –¥–∞–ª—å—à –∏–¥–∏–Ω–∏–∫—Ç —Ç...  \n",
       "2                         –º–æ–∂–µ—Ç –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑ –Ω–∞–¥–æ –±—ã–ª–æ  \n",
       "3            –∏–Ω—Ç–µ—Ä–µ—Å–Ω —á—Ç–æ –æ–Ω–∏ –∫—É—Ä –ø–µ—Ä–µ–¥ –∑–∞—Å–µ–¥–∞–Ω–∏—è–º–∏h  \n",
       "4  –Ω–µ –∑–Ω–∞ –≥–¥–µ –≤–∞—Å –Ω–∞ –¥—Ä—É–∂–±–µ —É –Ω–∞—Å –Ω–∞ –ª–æ–º–æ–Ω–æ—Å–æ–≤ —Å–æ...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY_train.loc[:, ['text', 'text_stemmed']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def search_best_tfidf_and_algo(X, y, algo = 'logit', ns = 3, num_of_evals = 10):\n",
    "    \n",
    "    def hyperopt_tfidf_algo_score(params):\n",
    "        try:\n",
    "            transf = TfidfVectorizer(stop_words = params['stop_words'], ngram_range = (1, params['ngram_range']),\n",
    "                                     min_df = params['min_df'], max_df = params['max_df'])\n",
    "\n",
    "            X_prep = transf.fit_transform(X.text_stemmed) if params['stemming'] else transf.fit_transform(X.text)\n",
    "\n",
    "            extra = ['exclamation_num', 'question_num']\n",
    "            X_prep = hstack([X_prep, X.loc[:, extra].values]) if params['activate_s'] else X_prep\n",
    "\n",
    "            algo_params = {key: value for key, value in params.items() if key not in ['stop_words', 'ngram_range', \n",
    "                                                                                      'stemming', 'activate_s',\n",
    "                                                                                      'min_df']}\n",
    "            if algo == 'logit':\n",
    "                clf = LogisticRegression(**algo_params, class_weight = 'balanced')\n",
    "            elif algo == 'xgb':\n",
    "                clf = XGBClassifier(objective = 'binary:logistic', **algo_params)\n",
    "\n",
    "            current_score_scores = cross_val_score(clf, X_prep, y, cv = StratifiedKFold(n_splits = ns),\n",
    "                                                   scoring = 'average_precision')\n",
    "            \n",
    "            mean_score = np.mean(current_score_scores)\n",
    "            current_score = mean_score if np.std(current_score_scores * 100) < 1.25 else mean_score / 1.25\n",
    "            \n",
    "        except:\n",
    "            current_score = 0\n",
    "            print(f\"Bad: {algo_params['min_df'], algo_params['max_df']}\")\n",
    "        \n",
    "        return -current_score\n",
    "        \n",
    "    space_tfidf = {\n",
    "        'stop_words': hp.choice('stop_words', [stopWords, None]),\n",
    "        'stemming': hp.choice('stemming', [True, False]),\n",
    "        'ngram_range': hp.choice('ngram_range', [1, 2]),\n",
    "        'min_df': hp.choice('min_df', np.arange(1, 4, 1)),\n",
    "        'activate_s': hp.choice('activate_s', [True, False]),\n",
    "    }\n",
    "    \n",
    "    if (algo == 'logit'):\n",
    "        space_algo = {\n",
    "            'C': hp.uniform('C', 10**(-3), 10**(2))\n",
    "        }\n",
    "#     elif algo == 'rf':\n",
    "#         space_algo = {\n",
    "#             'max_depth': hp.choice('max_depth', np.arange(11, 17, 1)),\n",
    "#             'max_features': hp.uniform('max_features', 0.1, 0.5),\n",
    "#             'max_samples': hp.uniform('max_samples', 0.4, 0.7),\n",
    "#             'n_estimators': hp.choice('n_estimators', np.arange(100, 401, 50)),\n",
    "#         }\n",
    "    elif algo == 'xgb':\n",
    "        space_algo = {\n",
    "            'learning_rate': hp.uniform('learning_rate', 0.01, 0.20),\n",
    "            'max_depth': hp.choice('max_depth', np.arange(3, 9, 1)),\n",
    "            'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 0.7),\n",
    "            'subsample': hp.uniform('subsample', 0.4, 0.8),\n",
    "            'n_estimators': hp.choice('n_estimators', np.arange(100, 501, 100))\n",
    "        }\n",
    "#     elif algo == 'lgbm':\n",
    "#         space_algo = {\n",
    "#             'learning_rate': hp.uniform('learning_rate', 0.01, 0.20),\n",
    "#             'max_depth': hp.choice('max_depth', np.arange(5, 13, 1)),\n",
    "#             'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 0.7),\n",
    "#             'subsample': hp.uniform('subsample', 0.4, 0.8),\n",
    "#             'n_estimators': hp.choice('n_estimators', np.arange(100, 401, 50)),\n",
    "#         }\n",
    "    else:\n",
    "        return 'Error: wrong algo'\n",
    "    \n",
    "    space = dict(**space_tfidf, **space_algo)\n",
    "    \n",
    "    best = fmin(fn = hyperopt_tfidf_algo_score, space = space, algo = tpe.suggest, max_evals = num_of_evals)\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def build_best_tfidf_and_algo(params, algo, X_train, y_train, X_valid, y_valid):\n",
    "    stops = [stopWords, None][params['stop_words']]\n",
    "    stemming = [True, False][params['stemming']]\n",
    "    activate_s = [True, False][params['activate_s']]\n",
    "    ngram_upper = [1, 2][params['ngram_range']]\n",
    "    \n",
    "    transformer = TfidfVectorizer(stop_words = stops, ngram_range = (1, ngram_upper),\n",
    "                                  min_df = params['min_df'], max_df = params['max_df'])\n",
    "    \n",
    "    if stemming:\n",
    "        X_train_prep = transformer.fit_transform(X_train.text_stemmed)\n",
    "        X_valid_prep = transformer.transform(X_valid.text_stemmed)\n",
    "    else:\n",
    "        X_train_prep = transformer.fit_transform(X_train.text)\n",
    "        X_valid_prep = transformer.transform(X_valid.text)\n",
    "        \n",
    "    extra = ['exclamation_num', 'question_num']\n",
    "    X_train_prep = hstack([X_train_prep, X_train.loc[:, extra].values]) if activate_s else X_train_prep\n",
    "    X_valid_prep = hstack([X_valid_prep, X_valid.loc[:, extra].values]) if activate_s else X_valid_prep\n",
    "    \n",
    "    algo_params = {key: value for key, value in params.items() if key not in ['stop_words', 'ngram_range',\n",
    "                                                                              'stemming', 'activate_s',\n",
    "                                                                              'min_df', 'max_df']}\n",
    "    if algo == 'logit':\n",
    "        clf = LogisticRegression(**algo_params, class_weight = 'balanced')\n",
    "    elif algo == 'xgb':\n",
    "        algo_params['n_estimators'] = np.arange(100, 401, 50)[algo_params['n_estimators']]\n",
    "        algo_params['max_depth'] = np.arange(3, 9, 1)[algo_params['max_depth']]\n",
    "        clf = XGBClassifier(objective = 'binary:logistic', **algo_params)\n",
    "    else:\n",
    "        return 'Error: wrong algo', None\n",
    "    \n",
    "    clf.fit(X_train_prep, y_train)\n",
    "    \n",
    "    aps_train = average_precision_score(y_true = y_train, y_score = clf.predict_proba(X_train_prep)[:, 1])\n",
    "    aps_valid = average_precision_score(y_true = y_valid, y_score = clf.predict_proba(X_valid_prep)[:, 1])\n",
    "    \n",
    "    return {'transformer': transformer, 'stemming': stemming, 'punctuations': activate_s, \n",
    "            'classifier': clf, 'scores': [aps_train, aps_valid]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_build(XY, label, algo, balancing = True, test_size = 0.2, ns = 5, num_of_evals = 10):\n",
    "    \n",
    "    if balancing:\n",
    "        sample_num = min(XY.loc[XY[label] == 1].shape[0], XY.loc[XY[label] == 0].shape[0])\n",
    "        XY_balanced = pd.concat([XY.loc[XY[label] == 1].sample(sample_num, random_state = 42),\n",
    "                                 XY.loc[XY[label] == 0].sample(sample_num, random_state = 42)], axis = 0)\n",
    "    else:\n",
    "        XY_balanced = XY\n",
    "    \n",
    "    XY_train, XY_valid = train_test_split(XY_balanced, stratify = XY_balanced[label], test_size = test_size,\n",
    "                                          shuffle = True, random_state = 42)\n",
    "    XY_train.reset_index(drop = True, inplace = True)\n",
    "    XY_valid.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    X_train = XY_train.drop(columns = label)\n",
    "    y_train = XY_train.loc[:, label].values\n",
    "    \n",
    "    X_valid = XY_valid.drop(columns = label)\n",
    "    y_valid = XY_valid.loc[:, label].values\n",
    "    \n",
    "    params = search_best_tfidf_and_algo(X_train, y_train, algo, ns = ns, num_of_evals = num_of_evals)\n",
    "    \n",
    "    if isinstance(params, str):\n",
    "        print(params)\n",
    "        return None, None\n",
    "    \n",
    "    return build_best_tfidf_and_algo(params, algo, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_algos = {}\n",
    "# activates_s = {}\n",
    "# final_algos['normal'], activates_s['normal'] = search_build(XY, 'normal', 'logit', balancing = False, \n",
    "#                                                             test_size = 0.2, ns = 3, num_of_evals = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# {'normal': {'transformer': TfidfVectorizer(max_df=0.9936050241211823, min_df=0.00043729649291974977,\n",
    "#                   ngram_range=(1, 2),\n",
    "#                   stop_words=['–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å',\n",
    "#                               '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ',\n",
    "#                               '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã',\n",
    "#                               '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', ...]),\n",
    "#   'stemming': False,\n",
    "#   'classifier': LogisticRegression(C=292.63650502520727),\n",
    "#   'scores': [0.9843409838416104, 0.9778704927054227]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monster_search(XY: pd.DataFrame, labels: list = ['insult', 'threat', 'obscenity'], balancing: list = [True] * 3,\n",
    "                   algos: list = ['logit', 'rf', 'xgb', 'lgbm'], test_size: float = 0.2,\n",
    "                   ns: list = [10] * 3, num_of_evals: list = [[10] * 4] * 3, final_algos: dict = {}) -> dict:\n",
    "    \n",
    "    for l, label in enumerate(labels):\n",
    "        print()\n",
    "        print('---------------------------------------------------------------------------------------------------')\n",
    "        print(label)\n",
    "        print('---------------------------------------------------------------------------------------------------')\n",
    "        \n",
    "        best_algo, best_valid_score = None, 0\n",
    "        for a, algo in enumerate(algos):\n",
    "            algo_result = search_build(XY, label, algo, balancing = balancing[l], \n",
    "                                       test_size = test_size, ns = ns[l], num_of_evals = num_of_evals[l][a])\n",
    "            \n",
    "            print(algo_result)\n",
    "            if (not best_valid_score) or (algo_result['scores'][1] > best_valid_score):\n",
    "                best_valid_score = algo_result['scores'][1]\n",
    "                best_algo = algo_result\n",
    "        \n",
    "        final_algos[label] = best_algo\n",
    "    return final_algos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18547, 9)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY_train_abn = XY_train.loc[XY_train.normal == 0, :]\n",
    "XY_train_abn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# final_algos_1 = monster_search(XY_train, labels = ['normal'], balancing = [True], algos = ['logit'],\n",
    "#                                test_size = 0.2, ns = [10],\n",
    "#                                num_of_evals = [[100]])\n",
    "\n",
    "# final_algos_1 = monster_search(XY_train_abn, labels = ['insult', 'threat', 'obscenity'], \n",
    "#                                balancing = [True, True, True], algos = ['logit', 'xgb'],\n",
    "#                                test_size = 0.2, ns = [10, 20, 20],\n",
    "#                                num_of_evals = [[100, 20], [200, 40], [200, 40]],\n",
    "#                                final_algos = final_algos_1)a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# final_algos_1\n",
    "# {'normal': {'transformer': TfidfVectorizer(max_df=0.9767372894638148, min_df=0.0005575599383208219,\n",
    "#                   ngram_range=(1, 2)),\n",
    "#   'stemming': True,\n",
    "#   'punctuations': False,\n",
    "#   'classifier': LogisticRegression(C=0.07896422086949316),\n",
    "#   'scores': [0.9426330195708196, 0.9302602690077265]},\n",
    "#  'insult': {'transformer': TfidfVectorizer(max_df=0.9513705996417339, min_df=0.0021129596423463902,\n",
    "#                   ngram_range=(1, 2)),\n",
    "#   'stemming': True,\n",
    "#   'punctuations': False,\n",
    "#   'classifier': LogisticRegression(C=0.09203285483490763),\n",
    "#   'scores': [0.9225223696701663, 0.893838261383933]},\n",
    "#  'threat': {'transformer': TfidfVectorizer(max_df=0.9745807994628432, min_df=0.0005067396168656215,\n",
    "#                   stop_words=['–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å',\n",
    "#                               '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ',\n",
    "#                               '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã',\n",
    "#                               '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', ...]),\n",
    "#   'stemming': False,\n",
    "#   'punctuations': True,\n",
    "#   'classifier': LogisticRegression(C=0.09260308200666084),\n",
    "#   'scores': [0.9380344268153942, 0.9333893110556957]},\n",
    "#  'obscenity': {'transformer': TfidfVectorizer(max_df=0.9849633250443195, min_df=0.0006991170600786703,\n",
    "#                   stop_words=['–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å',\n",
    "#                               '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ',\n",
    "#                               '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã',\n",
    "#                               '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', ...]),\n",
    "#   'stemming': True,\n",
    "#   'punctuations': True,\n",
    "#   'classifier': XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#                 colsample_bynode=1, colsample_bytree=0.6580496090346106, gamma=0,\n",
    "#                 gpu_id=-1, importance_type='gain', interaction_constraints='',\n",
    "#                 learning_rate=0.18382142639072288, max_delta_step=0, max_depth=8,\n",
    "#                 min_child_weight=1, missing=nan, monotone_constraints='()',\n",
    "#                 n_estimators=150, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
    "#                 reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "#                 subsample=0.5847539661247276, tree_method='exact',\n",
    "#                 validate_parameters=1, verbosity=None),\n",
    "#   'scores': [0.9778138916849568, 0.9424493109412991]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_algos_2 = monster_search(XY_train, labels = ['normal', 'insult', 'threat', 'obscenity'], \n",
    "#                                balancing = [False, False, False, False], algos = ['logit'],\n",
    "#                                test_size = 0.2, ns = [10, 10, 15, 15], \n",
    "#                                num_of_evals = [[100], [100], [100], [100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_predictions(data, algos):\n",
    "    \n",
    "    if algos['normal']['stemming']:\n",
    "        data_prep = algos['normal']['transformer'].transform(data.text_stemmed)\n",
    "    else:\n",
    "        data_prep = algos['normal']['transformer'].transform(data.text)\n",
    "    \n",
    "    X_train_n = hstack([data_prep, data.loc[:, ['exclamation_num', 'question_num']].values]) if \\\n",
    "                        algos['normal']['punctuations'] else data_prep\n",
    "    \n",
    "    preds = algos['normal']['classifier'].predict_proba(X_train_n)[:, 1].reshape(-1, 1)\n",
    "    predicted_labels_n = np.abs(algos['normal']['classifier'].predict(X_train_n) - 1).reshape(-1, 1)\n",
    "    \n",
    "    for label, algo in [(key, value) for key, value in algos.items() if key != 'normal']:\n",
    "        \n",
    "        if algos[label]['stemming']:\n",
    "            data_prep = algos[label]['transformer'].transform(data.text_stemmed)\n",
    "        else:\n",
    "            data_prep = algos[label]['transformer'].transform(data.text)\n",
    "        \n",
    "        X_train_abn = hstack([data_prep, data.loc[:, ['exclamation_num', 'question_num']].values]) if \\\n",
    "                              algos[label]['punctuations'] else data_prep\n",
    "        \n",
    "        preds_abn = algo['classifier'].predict_proba(X_train_abn)[:, 1].reshape(-1, 1) * predicted_labels_n\n",
    "        preds = np.concatenate([preds, preds_abn], axis = 1)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7411472516760014"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions = get_prob_predictions(XY_train, final_algos_1)\n",
    "# average_precision_score(y_true = XY_train.loc[:, list(final_algos_1.keys())],\n",
    "#                         y_score = predictions, average = 'macro')\n",
    "# 0.7411472516760014 –±–µ–∑ balanced logit, –Ω–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ –∏ —Ç–∞–∫ –µ—Å—Ç—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7255165203678371"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions = get_prob_predictions(XY_test, final_algos_1)\n",
    "# average_precision_score(y_true = XY_test.loc[:, list(final_algos_1.keys())], \n",
    "#                         y_score = predictions, average = 'macro')\n",
    "# 0.7255165203678371 –±–µ–∑ balanced logit, –Ω–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ –∏ —Ç–∞–∫ –µ—Å—Ç—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = get_prob_predictions(XY_train, final_algos_2)\n",
    "average_precision_score(y_true = XY_train.loc[:, list(final_algos_2.keys())],\n",
    "                        y_score = predictions, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = get_prob_predictions(XY_test, final_algos_2)\n",
    "average_precision_score(y_true = XY_test.loc[:, list(final_algos_2.keys())], \n",
    "                        y_score = predictions, average = 'macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public:  0.6514871975794686 (–±—ã–ª–∞ –æ—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏), –±—ã–ª–æ –±—ã –æ–∫–æ–ª–æ 0.72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = get_prob_predictions(XY_train, final_algos_2)\n",
    "average_precision_score(y_true = XY_train.loc[:, list(final_algos_2.keys())],\n",
    "                        y_score = predictions, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = get_prob_predictions(XY_test, final_algos_2)\n",
    "average_precision_score(y_true = XY_test.loc[:, list(final_algos_2.keys())], \n",
    "                        y_score = predictions, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt', 'r') as f:\n",
    "    test_ids = [s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.txt') as f:\n",
    "    test_data = [s.strip().split('\\t') for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('test.txt', 'r') as f:\n",
    "#     test_ids = [s.strip() for s in f.readlines()]\n",
    "\n",
    "# with open('data.txt') as f:\n",
    "#     test_data = [s.strip().split('\\t') for s in f.readlines()]\n",
    "\n",
    "# X_final_test = pd.DataFrame(test_ids, columns = ['id']).merge(pd.DataFrame(test_data, columns = ['id', 'text']), \n",
    "#                                                               how = 'left')\n",
    "# X_final_test['text_stemmed'] = stem_words(X_final_test['text'])\n",
    "# X_final_test['text_lemmatised'] = lemmatise_words(X_final_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final_test = pd.DataFrame(test_ids, columns = ['id']).merge(pd.DataFrame(test_data, columns = ['id', 'text']), \n",
    "                                                              how = 'left')\n",
    "X_final_test['text_stemmed'] = stem_words(X_final_test['text'])\n",
    "X_final_test['exclamation_num'] = X_final_test.text.str.count('!')\n",
    "X_final_test['question_num'] = X_final_test.text.str.count('\\?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>exclamation_num</th>\n",
       "      <th>question_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167315</td>\n",
       "      <td>–∫–∞–∫–∞—è –ø—Ä–µ–ª–µ—Å—Ç—å!!!üòç</td>\n",
       "      <td>–∫–∞–∫–∞—è –ø—Ä–µ–ª–µ—Å—Ç—åüòç</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>224546</td>\n",
       "      <td>–∫–∞–∞–ª –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤—å—é?</td>\n",
       "      <td>–∫–∞–∞ –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>241309</td>\n",
       "      <td>–≥–Ω–æ–π–Ω—ã–µ –ø–∏–¥–æ—Ä—ã –∞–ª–ª—ã –æ–Ω–∏</td>\n",
       "      <td>–≥–Ω–æ–π–Ω –ø–∏–¥–æ—Ä –∞–ª–ª –æ–Ω–∏</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31170</td>\n",
       "      <td>—á—ë —Ç—ã –≥—É–±—ã —à–ª—ë—à—å –≤ –ø–æ–º–∞–¥–µ?—Ñ—É –±–ª—è–¥—å</td>\n",
       "      <td>—á–µ —Ç—ã –≥—É–± —à–ª–µ—à –≤ –ø–æ–º–∞–¥–µ—Ñ –±–ª—è–¥</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173358</td>\n",
       "      <td>–º–∞—Ç—Ä–æ–Ω–∞ –ø–æ–º–æ–≥–∞–µ—Ç —Ä–µ–∞–ª—å–Ω–æ —ç—Ç–æ –ø—Ä–∞–≤–¥–∞. —Å–∞–º–∞ –∫ –Ω–µ...</td>\n",
       "      <td>–º–∞—Ç—Ä–æ–Ω –ø–æ–º–æ–≥–∞ —Ä–µ–∞–ª—å–Ω —ç—Ç –ø—Ä–∞–≤–¥ —Å–∞–º –∫ –Ω–µ–π –µ–∑–¥ –Ω–∞...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  \\\n",
       "0  167315                                 –∫–∞–∫–∞—è –ø—Ä–µ–ª–µ—Å—Ç—å!!!üòç   \n",
       "1  224546                            –∫–∞–∞–ª –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤—å—é?   \n",
       "2  241309                            –≥–Ω–æ–π–Ω—ã–µ –ø–∏–¥–æ—Ä—ã –∞–ª–ª—ã –æ–Ω–∏   \n",
       "3   31170                 —á—ë —Ç—ã –≥—É–±—ã —à–ª—ë—à—å –≤ –ø–æ–º–∞–¥–µ?—Ñ—É –±–ª—è–¥—å   \n",
       "4  173358  –º–∞—Ç—Ä–æ–Ω–∞ –ø–æ–º–æ–≥–∞–µ—Ç —Ä–µ–∞–ª—å–Ω–æ —ç—Ç–æ –ø—Ä–∞–≤–¥–∞. —Å–∞–º–∞ –∫ –Ω–µ...   \n",
       "\n",
       "                                        text_stemmed  exclamation_num  \\\n",
       "0                                    –∫–∞–∫–∞—è –ø—Ä–µ–ª–µ—Å—Ç—åüòç                3   \n",
       "1                                –∫–∞–∞ –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤                0   \n",
       "2                                –≥–Ω–æ–π–Ω –ø–∏–¥–æ—Ä –∞–ª–ª –æ–Ω–∏                0   \n",
       "3                      —á–µ —Ç—ã –≥—É–± —à–ª–µ—à –≤ –ø–æ–º–∞–¥–µ—Ñ –±–ª—è–¥                0   \n",
       "4  –º–∞—Ç—Ä–æ–Ω –ø–æ–º–æ–≥–∞ —Ä–µ–∞–ª—å–Ω —ç—Ç –ø—Ä–∞–≤–¥ —Å–∞–º –∫ –Ω–µ–π –µ–∑–¥ –Ω–∞...                0   \n",
       "\n",
       "   question_num  \n",
       "0             0  \n",
       "1             1  \n",
       "2             0  \n",
       "3             1  \n",
       "4             0  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = get_prob_predictions(X_final_test, final_algos_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_frame = pd.concat([pd.DataFrame(X_final_test.id.values, columns = ['id']), \n",
    "                         pd.DataFrame(predictions, columns = list(final_algos_1.keys()))], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final_frame.loc[:, ['id', 'normal', 'insult', 'obscenity', 'threat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('result', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predictions = get_prob_predictions(XY, loaded_model, activates_s)\n",
    "# average_precision_score(y_true = XY.loc[:, ['normal', 'insult', 'threat', 'obscenity']],\n",
    "#                         y_score = predictions, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(final_algos, open('final_algos', 'wb'))\n",
    "# pickle.dump(activates_s, open('activates_s', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_models = pickle.load(open('final_algos', 'rb'))\n",
    "# loaded_activates_s = pickle.load(open('activates_s', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text feature extraction and dimension reduction - level 2\n",
    "# trans_2_150_15 = CountVectorizer(min_df = 150, ngram_range = (1, 5), stop_words = stopwords.words('russian'))\n",
    "# text_train_2 = trans_2_150_15.fit_transform(XY_train.text_lemmatised)\n",
    "# text_test_2 = trans_2_150_15.transform(XY_test.text_lemmatised)\n",
    "\n",
    "# nmf_2_100 = NMF(n_components = 100, init = 'random', random_state = 0)\n",
    "# text_nmf_train_2 = nmf_2_100.fit_transform(text_train_2)\n",
    "# text_nmf_test_2 = nmf_2_100.transform(text_test_2)\n",
    "\n",
    "# pca_2_100 = TruncatedSVD(n_components = 100)\n",
    "# text_pca_train_2 = pca_2_100.fit_transform(text_train_2)\n",
    "# text_pca_test_2 = pca_2_100.transform(text_test_2)\n",
    "\n",
    "# trans_2_250_13 = CountVectorizer(max_features = 250, ngram_range = (1, 3), stop_words = stopwords.words('russian'))\n",
    "# text_train_2 = trans_2_250_13.fit_transform(XY_train.text_lemmatised)\n",
    "# text_test_2 = trans_2_250_13.transform(XY_test.text_lemmatised)\n",
    "\n",
    "# concat 2-level\n",
    "# train_2 = np.concatenate([pred_train_normal_bool_1, text_nmf_train_2, text_pca_train_2], axis = 1)\n",
    "# test_2 = np.concatenate([pred_test_normal_bool_1, text_nmf_test_2, text_pca_test_2], axis = 1)\n",
    "\n",
    "# train_2 = np.concatenate([pred_train_normal_bool_1, text_train_2.toarray()], axis = 1)\n",
    "# test_2 = np.concatenate([pred_test_normal_bool_1, text_test_2.toarray()], axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
