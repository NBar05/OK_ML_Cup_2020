{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "srmZrir30OL5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import string\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "ZzKSezAp0OMJ",
    "outputId": "93852034-0e37-4273-c11d-2de43a378543",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_lemmatised</th>\n",
       "      <th>normal</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>obscenity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41127</td>\n",
       "      <td>–¥–≤–æ—Ä–Ω–∏–∫–∞ –Ω–∞–¥–æ —Ç–æ–∂–µ —É–Ω–∏—á—Ç–æ–∂–∏—Ç—å!</td>\n",
       "      <td>–¥–≤–æ—Ä–Ω–∏–∫ –Ω–∞–¥–æ —Ç–æ–∂–µ —É–Ω–∏—á—Ç–æ–∂</td>\n",
       "      <td>–¥–≤–æ—Ä–Ω–∏–∫ –Ω–∞–¥–æ —Ç–æ–∂–µ —É–Ω–∏—á—Ç–æ–∂–∏—Ç—å</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6812</td>\n",
       "      <td>–º–æ—è —Å—Ç–∞—Ä—à–∞—è –Ω–µ–¥–µ–ª—é —à–∏–ø–µ–ª–∞, –Ω–µ –ø—Ä–∏–Ω–∏–º–∞–ª–∞ –ø–æ–¥–∫–∏–¥...</td>\n",
       "      <td>–º–æ—è —Å—Ç–∞—Ä—à –Ω–µ–¥–µ–ª —à–∏–ø–µ–ª –Ω–µ –ø—Ä–∏–Ω–∏–º–∞ –ø–æ–¥–∫–∏–¥—ã—à –∫–æ—Ç–æ...</td>\n",
       "      <td>–º–æ–π —Å—Ç–∞—Ä—à–∏–π –Ω–µ–¥–µ–ª—è —à–∏–ø–µ—Ç—å –Ω–µ –ø—Ä–∏–Ω–∏–º–∞—Ç—å –ø–æ–¥–∫–∏–¥—ã...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6256</td>\n",
       "      <td>–ø–æ–ª–Ω–æ—Å—Ç—å—é —Å –≤–∞–º–∏ —Å–æ–≥–ª–∞—Å–Ω–∞!</td>\n",
       "      <td>–ø–æ–ª–Ω–æ—Å—Ç —Å –≤–∞–º —Å–æ–≥–ª–∞—Å–Ω</td>\n",
       "      <td>–ø–æ–ª–Ω–æ—Å—Ç—å—é —Å –≤—ã —Å–æ–≥–ª–∞—Å–Ω—ã–π</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  \\\n",
       "0  41127                     –¥–≤–æ—Ä–Ω–∏–∫–∞ –Ω–∞–¥–æ —Ç–æ–∂–µ —É–Ω–∏—á—Ç–æ–∂–∏—Ç—å!   \n",
       "1   6812  –º–æ—è —Å—Ç–∞—Ä—à–∞—è –Ω–µ–¥–µ–ª—é —à–∏–ø–µ–ª–∞, –Ω–µ –ø—Ä–∏–Ω–∏–º–∞–ª–∞ –ø–æ–¥–∫–∏–¥...   \n",
       "2   6256                         –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å –≤–∞–º–∏ —Å–æ–≥–ª–∞—Å–Ω–∞!   \n",
       "\n",
       "                                        text_stemmed  \\\n",
       "0                          –¥–≤–æ—Ä–Ω–∏–∫ –Ω–∞–¥–æ —Ç–æ–∂–µ —É–Ω–∏—á—Ç–æ–∂   \n",
       "1  –º–æ—è —Å—Ç–∞—Ä—à –Ω–µ–¥–µ–ª —à–∏–ø–µ–ª –Ω–µ –ø—Ä–∏–Ω–∏–º–∞ –ø–æ–¥–∫–∏–¥—ã—à –∫–æ—Ç–æ...   \n",
       "2                              –ø–æ–ª–Ω–æ—Å—Ç —Å –≤–∞–º —Å–æ–≥–ª–∞—Å–Ω   \n",
       "\n",
       "                                     text_lemmatised  normal  threat  insult  \\\n",
       "0                       –¥–≤–æ—Ä–Ω–∏–∫ –Ω–∞–¥–æ —Ç–æ–∂–µ —É–Ω–∏—á—Ç–æ–∂–∏—Ç—å       0       1       0   \n",
       "1  –º–æ–π —Å—Ç–∞—Ä—à–∏–π –Ω–µ–¥–µ–ª—è —à–∏–ø–µ—Ç—å –Ω–µ –ø—Ä–∏–Ω–∏–º–∞—Ç—å –ø–æ–¥–∫–∏–¥—ã...       1       0       0   \n",
       "2                           –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å –≤—ã —Å–æ–≥–ª–∞—Å–Ω—ã–π       1       0       0   \n",
       "\n",
       "   obscenity  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY = pd.read_csv('XY.csv', header = 0)\n",
    "XY.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4KCI2T-A0OMT",
    "outputId": "f130e329-2974-4ac2-f735-629b64c0cbdd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((104142, 10), (18547, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY['exclamation_num'] = XY.text.str.count('!')\n",
    "XY['question_num'] = XY.text.str.count('\\?')\n",
    "\n",
    "XY_train, XY_test = train_test_split(XY, test_size = 0.3, shuffle = True, random_state = 42)\n",
    "XY_train.reset_index(drop = True, inplace = True)\n",
    "XY_test.reset_index(drop = True, inplace = True)\n",
    "XY_train_abn = XY_train.loc[XY_train.normal == 0, :].reset_index(drop = True)\n",
    "XY_train.shape, XY_train_abn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GTRux1Go0OMg"
   },
   "outputs": [],
   "source": [
    "def hyperopt_tdidf_logit_label(label):\n",
    "    \n",
    "    X = XY_train.text\n",
    "    y = XY_train[label]\n",
    "    \n",
    "    @ignore_warnings(category=ConvergenceWarning)\n",
    "    def hyperopt_tdidf_logit(params):\n",
    "        \n",
    "        pipe = Pipeline([('trans', TfidfVectorizer(min_df = 2)), ('clf', LogisticRegression(**params))])\n",
    "\n",
    "        score = cross_val_score(estimator = pipe, X = X, y = y,\n",
    "                                cv = StratifiedKFold(n_splits = 7), scoring = 'average_precision')\n",
    "\n",
    "        score_mean = score.mean()\n",
    "\n",
    "        return -score_mean\n",
    "\n",
    "    space_tfidf_logit = {'C': hp.uniform('C', 1, 25)}\n",
    "    \n",
    "    best = fmin(fn = hyperopt_tdidf_logit, space = space_tfidf_logit, algo = tpe.suggest, max_evals = 50)\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    pipe = Pipeline([('trans', TfidfVectorizer(min_df = 2)), ('clf', LogisticRegression(**best))])\n",
    "    pipe.fit(X, y)\n",
    "    \n",
    "    print(average_precision_score(y_score = pipe.predict_proba(X)[:, 1], y_true = y))\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OlRwm1Vh0OMm",
    "outputId": "ef24e8fe-53db-46c3-84ec-c97061d710d4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# algs_1 = dict()\n",
    "# for label in ['normal', 'insult', 'obscenity', 'threat']:\n",
    "#     print('----------------------------------------------------------------------------------------------------')\n",
    "#     print(label)\n",
    "#     print('----------------------------------------------------------------------------------------------------')\n",
    "#     print()\n",
    "#     algs_1[label] = hyperopt_tdidf_logit_label(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJRsdRRMOQnZ",
    "outputId": "04af64bf-64b6-4feb-c2ad-1db27fae4e36",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# algs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.36 s, sys: 70.9 ms, total: 3.43 s\n",
      "Wall time: 3.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# text feature extraction - level 1\n",
    "trans_1_2_11 = TfidfVectorizer(min_df = 2)\n",
    "text_train_1 = trans_1_2_11.fit_transform(XY_train.text)\n",
    "text_test_1 = trans_1_2_11.transform(XY_test.text)\n",
    "\n",
    "# classifier - level 1\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "# clf_1_normal = LogisticRegression(C = 5.3).fit(text_train_1, XY_train['normal'])\n",
    "# clf_1_insult = LogisticRegression(C = 12).fit(text_train_1, XY_train['insult'])\n",
    "# clf_1_threat = LogisticRegression(C = 10.6).fit(text_train_1, XY_train['threat'])\n",
    "# clf_1_obscenity = LogisticRegression(C = 21.8).fit(text_train_1, XY_train['obscenity'])\n",
    "\n",
    "# pred_train_normal_1 = clf_1_normal.predict_proba(text_train_1)[:, 1].reshape(-1, 1)\n",
    "# pred_train_insult_1 = clf_1_insult.predict_proba(text_train_1)[:, 1].reshape(-1, 1)\n",
    "# pred_train_threat_1 = clf_1_threat.predict_proba(text_train_1)[:, 1].reshape(-1, 1)\n",
    "# pred_train_obscenity_1 = clf_1_obscenity.predict_proba(text_train_1)[:, 1].reshape(-1, 1)\n",
    "\n",
    "# pred_test_normal_1 = clf_1_normal.predict_proba(text_test_1)[:, 1].reshape(-1, 1)\n",
    "# pred_test_insult_1 = clf_1_insult.predict_proba(text_test_1)[:, 1].reshape(-1, 1)\n",
    "# pred_test_threat_1 = clf_1_threat.predict_proba(text_test_1)[:, 1].reshape(-1, 1)\n",
    "# pred_test_obscenity_1 = clf_1_obscenity.predict_proba(text_test_1)[:, 1].reshape(-1, 1)\n",
    "\n",
    "# text feature extraction - level 2\n",
    "stopWords = stopwords.words('russian') + ['—ç—Ç–æ', '–≤—Å—ë', '–≤–µ—Å—å', '–µ—â—ë', '—á–µ–ª–æ–≤–µ–∫', '—Ç–≤–æ–π', '–∫–æ—Ç–æ—Ä—ã–π', '–∏–¥—Ç–∏', '—Å—É–¥',\n",
    "                                          '—Å–≤–æ–π', '—Ä—É–∫–∞', '–Ω—É–∂–Ω–æ', '—Ä–µ–±—ë–Ω–æ–∫', '–µ—ë', '–∂–∏—Ç—å', '–ø—Ä–æ—Å—Ç–æ', '–Ω–∞—à', '–≤–∞—à',\n",
    "                                          '—Ä–æ—Å—Å–∏—è', '—Å—Ç—Ä–∞–Ω–∞', '–º–æ—á—å', '–Ω–∞—Ä–æ–¥', '–ø—É—Ç–∏–Ω', '–ø—É—Ç–∏–Ω—Å–∫–∏–π', '—Ä–æ—Å—Å–∏—è',\n",
    "                                          '–Ω–æ–≥–∞', '–∂–µ–Ω–∞', '–º–µ—Å—Ç–æ', '–º—É–∂–∏–∫', '–¥–∞–ª—ë–∫–∏–π', '–º–∞–º–∞', '–¥–µ–Ω—å', '—Å–∫–∞–∑–∞—Ç—å',\n",
    "                                          '–∫–∞–∂–¥—ã–π', '–ø—É—Å—Ç—å', '–¥–µ–ª–∞—Ç—å', '–ª—é–±–∏—Ç—å', '–∑–Ω–∞—Ç—å', '—Ö–æ—Ä–æ—à–∏–π', '–±–æ–ª—å—à–æ–π',\n",
    "                                          '–∑–µ–º–ª—è', '—Å–ª–æ–≤–æ', '–Ω–∞–π—Ç–∏', '—Å—Ç–µ–Ω–∫–∞', '–≤–º–µ—Å—Ç–µ', '–≤–∑—è—Ç—å', '—Å–∞–º—ã–π', '—è–π—Ü–æ',\n",
    "                                          '—Å–∫–æ–ª—å–∫–æ', '—Å–º–æ—Ç—Ä–µ—Ç—å', '—Å–¥–µ–ª–∞—Ç—å', '–≥–æ–ª–æ–≤–∞', '–≥–æ–≤–æ—Ä–∏—Ç—å', '–≤–æ–æ–±—â–µ', '–≥–æ–¥',\n",
    "                                          '–¥–µ–Ω—å–≥–∞', '–ø—Ä–æ–¥–∞–∂–Ω—ã–π', '–ø–∏—Å–∞—Ç—å', '—Ä–∞–±–æ—Ç–∞—Ç—å', '–¥—É–º–∞—Ç—å', '–∂–∏–∑–Ω—å', '–º–æ–∑–≥',\n",
    "                                          '—Ä—É—Å—Å–∫–∏–π', '—Å—Ä–∞–∑—É', '–º–∞–ª–æ', '–ø–ª–æ—â–∞–¥—å', '—Å–æ–±–∞–∫–∞', '–µ—Å—Ç–∏', '—Ä–æ—Ç', '—Ö–æ—Ç–µ—Ç—å',\n",
    "                                          '–¥–∞–≤–∞—Ç—å', '–º–∞—Ç—å', '–≤—ã–µ—Å—Ç–∏', '—Å–∏–¥–µ—Ç—å', '–ø–æ–π—Ç–∏', '–¥–∞—Ç—å', '–¥–∞–≤–Ω–æ', '—Å–∞–∂–∞—Ç—å',\n",
    "                                          '–ø–æ–ª–Ω—ã–π', '–ø–æ—Ä–∞', '—Å—Ç–∞—Ç—å', '–¥–æ–ª–∂–Ω—ã–π', '—Å—Ç–∞—Ç—å', '–≤—Ä–µ–º—è', '–ø–æ–∫–∞', '–≤–ª–∞—Å—Ç—å',\n",
    "                                          '–Ω–∏–∫—Ç–æ', '–ø—Ä–∏–≤—è–∑–∞—Ç—å', '–±–æ–≥', '—Å–∫–æ—Ä–æ', '–∫–æ—Ä–º–∏—Ç—å' '–≤—Ä–∞–≥', '—à–µ—è', '–±–∞—à–∫–∞', \n",
    "                                          '–±–∞–±–∞', '–º—É–∂', '–ø–æ–∫–∞–∑–∞—Ç—å', '—É–∫—Ä–∞–∏–Ω–∞', '—Å—Ç–∞—Ä—ã–π', '—Ä–æ–¥–∏—Ç–µ–ª—å', '–ø–æ—Å–∞–¥–∏—Ç—å',\n",
    "                                          '–≤–∏–¥–µ—Ç—å', '–≤—Ä–∞–≥', '—Å—É–ø–µ—Ä', '–∂–µ–Ω—â–∏–Ω–∞', '—Å—Ç–æ–∏—Ç—å', '–∫–ª–∞—Å—Å–Ω—ã–π', '–ø–µ—Ä–≤—ã–π', \n",
    "                                          '–Ω–∞—á–∞—Ç—å', '–≤–∞–ª–∏—Ç—å', '–ø—Ä–µ–¥–∞—Ç–µ–ª—å', 'fr', 'fr fr', '—Å–ª–µ–¥—Å—Ç–≤–∏–µ', '–ø—Ä–∏–¥—É—Ä–æ–∫', \n",
    "                                          '–ø—Ä–∏–≤–µ—Ç', '–Ω—É–∂–Ω—ã–π', '—Ä–µ—à–∏—Ç—å', '8oi', '—á–µ', '–∫–æ—Ä–º–∏—Ç—å', '–¥—Ä—É–≥', '–¥—É—Ä–∞–∫', \n",
    "                                          '–ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å', '–¥–µ–ª–æ', '–æ–±–∞', '–ø–æ—á–µ–º—É', '–º–∏—Ä', '—É–º–µ—Ç—å','–æ—Ç–≤–µ—Ç–∏—Ç—å', '—Å–µ–º—å—è',\n",
    "                                          '—è–∑—ã–∫', '–≤–∏–¥–Ω–æ', '–±–æ—è—Ç—å—Å—è', '–¥–∞–≤–∏—Ç—å', '–≤—Å—è–∫–∏–π', '—Ö–æ–∑—è–∏–Ω', '–∫—Ä–∞—Å–∏–≤—ã–π', \n",
    "                                          '–≥–ª–∞–∑', '–¥–µ–≤–æ—á–∫–∞', '–ø–æ—Å—Ç–∞–≤–∏—Ç—å', '–º–∞–ª–µ–Ω—å–∫–∏–π', '—Ö–æ—Ç–µ—Ç—å—Å—è', '–æ—Å—Ç–∞–ª—å–Ω–æ–π',\n",
    "                                          '–æ—á–µ–Ω—å', '—Å—Ç–∞–ª–∏–Ω', '–∑–∞–∫–æ–Ω', '–Ω–∞–≤–µ—Ä–Ω–æ–µ', '–ø—Ä–∏–π—Ç–∏', '–∏–º–µ—Ç—å', '–∫–ª–∞—Å—Å', '—Ç—è',\n",
    "                                          '–Ω–æ—á—å', '–∂–¥–∞—Ç—å', '—Ç–∞–∫–∂–µ']\n",
    "# tf = TfidfVectorizer(min_df = 0.01, ngram_range = (1, 2), stop_words = stopWords)\n",
    "tf_insult = CountVectorizer(max_features = 50, ngram_range = (1, 2), \n",
    "                            stop_words = stopWords).fit(XY_train.loc[XY_train.insult == 1, 'text_lemmatised'])\n",
    "tf_threat = CountVectorizer(max_features = 75, ngram_range = (1, 2), \n",
    "                            stop_words = stopWords).fit(XY_train.loc[XY_train.threat == 1, 'text_lemmatised'])\n",
    "tf_obscenity = CountVectorizer(max_features = 75, ngram_range = (1, 2), \n",
    "                               stop_words = stopWords).fit(XY_train.loc[XY_train.obscenity == 1, 'text_lemmatised'])\n",
    "\n",
    "# text_train_insult_2 = (tf_insult.transform(XY_train.text_lemmatised).toarray() > 0).astype('int')\n",
    "# text_test_insult_2 = (tf_insult.transform(XY_test.text_lemmatised).toarray() > 0).astype('int')\n",
    "\n",
    "# text_train_threat_2 = (tf_threat.transform(XY_train.text_lemmatised).toarray() > 0).astype('int')\n",
    "# text_test_threat_2 = (tf_threat.transform(XY_test.text_lemmatised).toarray() > 0).astype('int')\n",
    "\n",
    "# text_train_obscenity_2 = (tf_obscenity.transform(XY_train.text_lemmatised).toarray() > 0).astype('int')\n",
    "# text_test_obscenity_2 = (tf_obscenity.transform(XY_test.text_lemmatised).toarray() > 0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–±—ã–¥–ª–æ',\n",
       " '–¥—Ä–æ—á–∏—Ç—å',\n",
       " '–∫–æ–Ω—á–∏—Ç—å',\n",
       " '–º–∏–Ω—É—Ç—å',\n",
       " '–æ—Ç—Ä–µ–∑–∞—Ç—å',\n",
       " '–µ–±–∞–ª',\n",
       " '–ø–∏–∑–¥–µ—Ü',\n",
       " '–≤–¥—É—Ç—å',\n",
       " '–ø–æ–ª–∏–∑–∞—Ç—å',\n",
       " '—Å–æ—Å–Ω—É—Ç—å',\n",
       " '–ø–æ–≤–µ—Å–∏—Ç—å',\n",
       " '—Å—ã–Ω',\n",
       " '–±—Ä–∞—Ç—å',\n",
       " '—É—Ç–æ–ø–∏—Ç—å',\n",
       " '–ø–∏–∑–¥–∞',\n",
       " '—Å–ø–µ—Ä–º–∞',\n",
       " '—Å–µ—Å—Ç—Ä–∞',\n",
       " '–∫–æ–∑—ë–ª',\n",
       " '–ø–∏–¥–æ—Ä–∞—Å',\n",
       " '–Ω–∞–±–∏—Ç—å',\n",
       " '–≥–æ–≤–Ω–æ',\n",
       " '–∑–∞–∫–æ–ø–∞—Ç—å',\n",
       " '—Ö–æ—Ö–æ–ª',\n",
       " '–≤–æ—Ä',\n",
       " '–¥—É—Ä–∞',\n",
       " '—É–±–ª—é–¥–æ–∫',\n",
       " '–∫–∞–∑–Ω–∏—Ç—å',\n",
       " '–≤—ã–µ–±–∞–ª',\n",
       " '–ª–∏–∑–∞—Ç—å',\n",
       " '–∑–∞—Å–∞–¥–∏—Ç—å',\n",
       " '—á–º–æ',\n",
       " '—Ä–∞—Å—Ç—Ä–µ–ª–∞',\n",
       " '–∑–∞–¥–Ω–∏—Ü–∞',\n",
       " '–ø—Ä–∏–ª—é–¥–Ω–æ',\n",
       " '–≥–Ω–∏–¥–∞',\n",
       " '—Å—É–∫–∞',\n",
       " '—Ä–∞—Å—Ç—Ä–µ–ª—è—Ç—å',\n",
       " '–Ω–∞—Å–æ—Å–∞—Ç—å',\n",
       " '—Å–æ—Å–∞—Ç—å —Å–æ—Å–∞—Ç—å',\n",
       " '–ø–∏–∑–¥–∏—Ç—å',\n",
       " '–ø—Ä–æ–∫–ª—è—Ç—ã–π',\n",
       " '–±–∏—Ç—å',\n",
       " '–æ—Ç—Å—Ç—Ä–µ–ª–∏–≤–∞—Ç—å',\n",
       " '–µ–±—É',\n",
       " '—É–±–∏–≤–∞—Ç—å',\n",
       " '–∫–æ–Ω—á–µ–Ω—ã–π',\n",
       " '–∂–∏–≤–æ—Ç–Ω–æ–µ',\n",
       " '—Å–µ–∫—Å',\n",
       " '–±–ª—è–¥—å',\n",
       " '–Ω–∞–ø–∏—Å–∞—Ç—å',\n",
       " '—à–ª—é—Ö–∞',\n",
       " '–æ—á–∫–æ',\n",
       " '—Ñ–∞—à–∏—Å—Ç',\n",
       " '–µ–±–µ—Ç–∞',\n",
       " '–æ—Ç—Ä—É–±–∏—Ç—å',\n",
       " '–ø–æ—Å–æ—Å–∞—Ç—å',\n",
       " '—Å–¥–æ—Ö–Ω—É—Ç—å',\n",
       " '—Å—Ç—Ä–µ–ª—è—Ç—å',\n",
       " '—É–Ω–∏—á—Ç–æ–∂–∞—Ç—å',\n",
       " '—Ä–∞–∫',\n",
       " '—Å–∂–µ—á—å',\n",
       " '–≥–∞–¥',\n",
       " '—Å–∏—Å—å–∫–∞',\n",
       " '—ë–±–∞–Ω—ã–π',\n",
       " '–ø—Ä–∏–±–∏—Ç—å',\n",
       " '—Ç—Ä–∞—Ö–∞—Ç',\n",
       " '–¥–µ–±–∏–ª',\n",
       " '—à–∞–ª–∞–≤—ã–π',\n",
       " '—Ä–∞—Å—Ç—Ä–µ–ª–∏–≤–∞—Ç—å',\n",
       " '–Ω–µ–≥—Ä',\n",
       " '–æ—Ç—Å–∞—Å—ã–≤–∞—Ç—å',\n",
       " '–æ—Ç–æ—Ä–≤–∞—Ç—å',\n",
       " '—Ö–µ—Ä',\n",
       " '—É—Ä–æ–¥',\n",
       " '–ø–∏–¥–∞—Ä–∞—Å',\n",
       " '–∫–∞—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å',\n",
       " '–ø–æ–ø',\n",
       " '–ø—Ä–∏—Å—Ç—Ä–µ–ª–∏—Ç—å',\n",
       " '–ø–∞–¥–ª–æ',\n",
       " '—Ç—Ä–∞—Ö–Ω—É—Ç—å',\n",
       " '–∑–∞—Å—É–Ω—É—Ç—å',\n",
       " '–º–æ—Ä–¥–∞',\n",
       " '–≤—ã–µ–±–∞—Ç—å',\n",
       " '–∂–æ–ø–∞',\n",
       " '—Å—Ä–∞–∫',\n",
       " '–∂–∏–≤—å—ë–º',\n",
       " '—Ö—É–π–Ω—è',\n",
       " '–ø–æ–Ω—è—Ç—å',\n",
       " '–º–æ—á–∏—Ç—å',\n",
       " '—Ä–∞—Å—Å—Ç—Ä–µ–ª—è—Ç—å',\n",
       " '–∞–¥',\n",
       " '—Ä–∞—Å—Å—Ç—Ä–µ–ª',\n",
       " '–µ–±—É—Ç',\n",
       " '—É–Ω–∏—á—Ç–æ–∂–∏—Ç—å',\n",
       " '–ø–æ–¥–≤–µ—Å–∏—Ç—å',\n",
       " '–≥–∞–Ω–¥–æ–Ω',\n",
       " '–º—Ä–∞–∑—å',\n",
       " '–ø–æ–¥—Ä–æ—á–∏—Ç—å',\n",
       " '—Ä–∞—Å—Å—Ç—Ä–µ–ª–∏–≤–∞—Ç—å',\n",
       " '–±–ª—è',\n",
       " '–ø–æ–≤–∞–¥–Ω–æ',\n",
       " '–æ—Ç–ª–∏–∑–∞—Ç—å',\n",
       " '–µ–±–∞–Ω—É—Ç—å',\n",
       " '–≤–µ—à–∞—Ç—å',\n",
       " '—Ö—É–π —Å–æ—Å–∞—Ç—å',\n",
       " '–≥–æ—Ä–µ—Ç—å',\n",
       " '–∫–æ–ª',\n",
       " '–ø–∏–¥–æ—Ä',\n",
       " '—Å–≤–æ–ª–æ—á—å',\n",
       " '–∏–¥–∏–æ—Ç',\n",
       " '–æ—Å—Ç–∞—Ç—å—Å—è',\n",
       " '–µ–±–∞—Ç—å',\n",
       " '–ø–µ—Ç—É—Ö',\n",
       " '–ø–∏—Å—è',\n",
       " '—Ä–æ—Ç–∏–∫',\n",
       " '–ø–æ–ø–∫–∞',\n",
       " '—Ö—Ä–µ–Ω',\n",
       " '—Å—É—á–∫–∞',\n",
       " '–Ω–∞–∫–∞–∑–∞—Ç—å',\n",
       " '—Ö—É–π',\n",
       " '–¥–æ–ª–±–æ–µ–±',\n",
       " '—Ç—é—Ä—å–º–∞',\n",
       " '–¥—ã—Ä–∫–∞',\n",
       " '–ø–æ–π–º–∞—Ç—å',\n",
       " '—Å–æ—Å–∞—Ç—å —Ö—É–π',\n",
       " '—Ç–æ—á–Ω–æ',\n",
       " '–Ω–∞—Ö—É–π',\n",
       " '—Ç—É–ø–æ–π',\n",
       " '–≤–æ–Ω—é—á–∏–π',\n",
       " '—Ç—Ä–∞—Ö–∞—Ç—å—Å—è',\n",
       " '—Ç—Ä–∞—Ö–∞—Ç—å',\n",
       " '—Ç–≤–∞—Ä—å',\n",
       " '—Ä–æ–∂–∞',\n",
       " '–¥–µ–≤—É—à–∫–∞',\n",
       " '—á–ª–µ–Ω',\n",
       " '–≥–æ—Ä–µ—Ç—å –∞–¥',\n",
       " '–æ—Ç—Å–æ—Å–∞—Ç—å',\n",
       " '–¥–æ–ª–±–æ–µ—Å—Ç–∏',\n",
       " '—Å–æ—Å–∞—Ç—å',\n",
       " '–ø–∏–¥–∞—Ä',\n",
       " '—É–±–∏—Ç—å',\n",
       " '—Å–º–µ—Ä—Ç—å']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(tf_insult.vocabulary_) | set(tf_threat.vocabulary_) | set(tf_obscenity.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:20<00:00, 24.43trial/s, best loss: -0.9909100993058516]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def mini_hyperopt(param):\n",
    "    y_pred = np.array(pred_train_normal_1 >= param['threshold'], dtype = 'int')\n",
    "    return -f1_score(y_pred = y_pred, y_true = XY_train['normal'])\n",
    "    \n",
    "space_thres = {'threshold': hp.uniform('threshold', 0, 1)}\n",
    "    \n",
    "best_mini = fmin(fn = mini_hyperopt, space = space_thres, algo = tpe.suggest, max_evals = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_normal_bool_1 = np.array(pred_train_normal_1 >= round(best_mini['threshold'], 3), dtype = 'int')\n",
    "pred_test_normal_bool_1 = np.array(pred_test_normal_1 >= round(best_mini['threshold'], 3), dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2_insult = np.concatenate([pred_train_insult_1, pred_train_normal_bool_1, text_train_insult_2], \n",
    "                                axis = 1)\n",
    "train_2_threat = np.concatenate([pred_train_threat_1, pred_train_normal_bool_1, text_train_threat_2], \n",
    "                                axis = 1)\n",
    "train_2_obscenity = np.concatenate([pred_train_obscenity_1, pred_train_normal_bool_1, text_train_obscenity_2], \n",
    "                                   axis = 1)\n",
    "\n",
    "test_2_insult = np.concatenate([pred_test_insult_1, pred_test_normal_bool_1, text_test_insult_2], \n",
    "                               axis = 1)\n",
    "test_2_threat = np.concatenate([pred_test_threat_1, pred_test_normal_bool_1, text_test_threat_2], \n",
    "                               axis = 1)\n",
    "test_2_obscenity = np.concatenate([pred_test_obscenity_1, pred_test_normal_bool_1, text_test_obscenity_2], \n",
    "                                  axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [03:37<00:00,  1.15trial/s, best loss: -0.9153770907605178]\n"
     ]
    }
   ],
   "source": [
    "def hyperopt_2(params):\n",
    "    \n",
    "    lg = LogisticRegression(**params).fit(train_2_insult, XY_train['insult'])\n",
    "    score = average_precision_score(y_score = lg.predict_proba(test_2_insult)[:, 1], y_true = XY_test['insult'])\n",
    "    \n",
    "    return -score\n",
    "    \n",
    "best_insult_C_2 = fmin(fn = hyperopt_2, space = {'C': hp.uniform('C', 0.01, 10),\n",
    "                                                 'class_weight': hp.choice('class_weight', [None, 'balanced'])},\n",
    "                       algo = tpe.suggest, max_evals = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.03578957898023842, 'class_weight': 'balanced'}"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_insult_C_2['class_weight'] = [None, 'balanced'][best_insult_C_2['class_weight']]\n",
    "best_insult_C_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lg = LogisticRegression(**best_insult_C_2).fit(train_2_insult, XY_train['insult'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9153770907605178"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision_score(y_score = lg.predict_proba(test_2_insult)[:, 1], y_true = XY_test['insult'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [03:15<00:00,  1.28trial/s, best loss: -0.8853170353601449]\n"
     ]
    }
   ],
   "source": [
    "def hyperopt_2(params):\n",
    "    \n",
    "    lg = LogisticRegression(**params).fit(train_2_threat, XY_train['threat'])\n",
    "    score = average_precision_score(y_score = lg.predict_proba(test_2_threat)[:, 1], y_true = XY_test['threat'])\n",
    "    \n",
    "    return -score\n",
    "    \n",
    "best_threat_C_2 = fmin(fn = hyperopt_2, space = {'C': hp.uniform('C', 0.01, 10),\n",
    "                                                 'class_weight': hp.choice('class_weight', [None, 'balanced'])},\n",
    "                       algo = tpe.suggest, max_evals = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.02527516977232648, 'class_weight': 'balanced'}"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_threat_C_2['class_weight'] = [None, 'balanced'][best_threat_C_2['class_weight']]\n",
    "best_threat_C_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression(**best_threat_C_2).fit(train_2_threat, XY_train['threat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8853170353601449"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision_score(y_score = lg.predict_proba(test_2_threat)[:, 1], y_true = XY_test['threat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [02:12<00:00,  1.89trial/s, best loss: -0.7729510611264273]\n"
     ]
    }
   ],
   "source": [
    "def hyperopt_2(params):\n",
    "    \n",
    "    lg = LogisticRegression(**params).fit(train_2_obscenity, XY_train['obscenity'])\n",
    "    score = average_precision_score(y_score = lg.predict_proba(test_2_obscenity)[:, 1], y_true = XY_test['obscenity'])\n",
    "    \n",
    "    return -score\n",
    "    \n",
    "best_obscenity_C_2 = fmin(fn = hyperopt_2, space = {'C': hp.uniform('C', 0.01, 10),\n",
    "                                                    'class_weight': hp.choice('class_weight', [None, 'balanced'])},\n",
    "                          algo = tpe.suggest, max_evals = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.2923206108631109, 'class_weight': None}"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_obscenity_C_2['class_weight'] = [None, 'balanced'][best_obscenity_C_2['class_weight']]\n",
    "best_obscenity_C_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression(**best_obscenity_C_2).fit(train_2_obscenity, XY_train['obscenity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7729510611264273"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision_score(y_score = lg.predict_proba(test_2_obscenity)[:, 1], y_true = XY_test['obscenity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "id": "UAexmbXV0ONc",
    "outputId": "8617ec54-4bec-4d7a-ddd2-589e23a22347",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_lemmatised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167315</td>\n",
       "      <td>–∫–∞–∫–∞—è –ø—Ä–µ–ª–µ—Å—Ç—å!!!üòç</td>\n",
       "      <td>–∫–∞–∫–∞—è –ø—Ä–µ–ª–µ—Å—Ç—åüòç</td>\n",
       "      <td>–∫–∞–∫–æ–π –ø—Ä–µ–ª–µ—Å—Ç—åüòç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>224546</td>\n",
       "      <td>–∫–∞–∞–ª –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤—å—é?</td>\n",
       "      <td>–∫–∞–∞ –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤</td>\n",
       "      <td>–∫–∞–∞–ª–∞ –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤—å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>241309</td>\n",
       "      <td>–≥–Ω–æ–π–Ω—ã–µ –ø–∏–¥–æ—Ä—ã –∞–ª–ª—ã –æ–Ω–∏</td>\n",
       "      <td>–≥–Ω–æ–π–Ω –ø–∏–¥–æ—Ä –∞–ª–ª –æ–Ω–∏</td>\n",
       "      <td>–≥–Ω–æ–π–Ω—ã–π –ø–∏–¥–æ—Ä –∞–ª–ª–∞ –æ–Ω–∏</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                     text         text_stemmed  \\\n",
       "0  167315       –∫–∞–∫–∞—è –ø—Ä–µ–ª–µ—Å—Ç—å!!!üòç      –∫–∞–∫–∞—è –ø—Ä–µ–ª–µ—Å—Ç—åüòç   \n",
       "1  224546  –∫–∞–∞–ª –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤—å—é?  –∫–∞–∞ –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤   \n",
       "2  241309  –≥–Ω–æ–π–Ω—ã–µ –ø–∏–¥–æ—Ä—ã –∞–ª–ª—ã –æ–Ω–∏  –≥–Ω–æ–π–Ω –ø–∏–¥–æ—Ä –∞–ª–ª –æ–Ω–∏   \n",
       "\n",
       "          text_lemmatised  \n",
       "0         –∫–∞–∫–æ–π –ø—Ä–µ–ª–µ—Å—Ç—åüòç  \n",
       "1  –∫–∞–∞–ª–∞ –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤—å  \n",
       "2  –≥–Ω–æ–π–Ω—ã–π –ø–∏–¥–æ—Ä –∞–ª–ª–∞ –æ–Ω–∏  "
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final_test = pd.read_csv('X_final_test.csv', header = 0)\n",
    "X_final_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "id": "HuiwWJcg0ONb",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:29<00:00, 17.08trial/s, best loss: -0.9914620782152145]\n"
     ]
    }
   ],
   "source": [
    "# text feature extraction - level 1\n",
    "trans_1_2_11 = TfidfVectorizer(min_df = 2)\n",
    "text_train_1 = trans_1_2_11.fit_transform(XY.text)\n",
    "text_test_1 = trans_1_2_11.transform(X_final_test.text)\n",
    "\n",
    "# classifier - level 1\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "clf_1_normal = LogisticRegression(C = 5.3).fit(text_train_1, XY['normal'])\n",
    "clf_1_insult = LogisticRegression(C = 12).fit(text_train_1, XY['insult'])\n",
    "clf_1_threat = LogisticRegression(C = 10.6).fit(text_train_1, XY['threat'])\n",
    "clf_1_obscenity = LogisticRegression(C = 21.8).fit(text_train_1, XY['obscenity'])\n",
    "\n",
    "pred_train_normal_1 = clf_1_normal.predict_proba(text_train_1)[:, 1].reshape(-1, 1)\n",
    "pred_train_insult_1 = clf_1_insult.predict_proba(text_train_1)[:, 1].reshape(-1, 1)\n",
    "pred_train_threat_1 = clf_1_threat.predict_proba(text_train_1)[:, 1].reshape(-1, 1)\n",
    "pred_train_obscenity_1 = clf_1_obscenity.predict_proba(text_train_1)[:, 1].reshape(-1, 1)\n",
    "\n",
    "pred_test_normal_1 = clf_1_normal.predict_proba(text_test_1)[:, 1].reshape(-1, 1)\n",
    "pred_test_insult_1 = clf_1_insult.predict_proba(text_test_1)[:, 1].reshape(-1, 1)\n",
    "pred_test_threat_1 = clf_1_threat.predict_proba(text_test_1)[:, 1].reshape(-1, 1)\n",
    "pred_test_obscenity_1 = clf_1_obscenity.predict_proba(text_test_1)[:, 1].reshape(-1, 1)\n",
    "\n",
    "# text feature extraction - level 2\n",
    "stopWords = stopwords.words('russian') + ['—ç—Ç–æ', '–≤—Å—ë', '–≤–µ—Å—å', '–µ—â—ë', '—á–µ–ª–æ–≤–µ–∫', '—Ç–≤–æ–π', '–∫–æ—Ç–æ—Ä—ã–π', '–∏–¥—Ç–∏', '—Å—É–¥',\n",
    "                                          '—Å–≤–æ–π', '—Ä—É–∫–∞', '–Ω—É–∂–Ω–æ', '—Ä–µ–±—ë–Ω–æ–∫', '–µ—ë', '–∂–∏—Ç—å', '–ø—Ä–æ—Å—Ç–æ', '–Ω–∞—à', '–≤–∞—à',\n",
    "                                          '—Ä–æ—Å—Å–∏—è', '—Å—Ç—Ä–∞–Ω–∞', '–º–æ—á—å', '–Ω–∞—Ä–æ–¥', '–ø—É—Ç–∏–Ω', '–ø—É—Ç–∏–Ω—Å–∫–∏–π', '—Ä–æ—Å—Å–∏—è',\n",
    "                                          '–Ω–æ–≥–∞', '–∂–µ–Ω–∞', '–º–µ—Å—Ç–æ', '–º—É–∂–∏–∫', '–¥–∞–ª—ë–∫–∏–π', '–º–∞–º–∞', '–¥–µ–Ω—å', '—Å–∫–∞–∑–∞—Ç—å',\n",
    "                                          '–∫–∞–∂–¥—ã–π', '–ø—É—Å—Ç—å', '–¥–µ–ª–∞—Ç—å', '–ª—é–±–∏—Ç—å', '–∑–Ω–∞—Ç—å', '—Ö–æ—Ä–æ—à–∏–π', '–±–æ–ª—å—à–æ–π',\n",
    "                                          '–∑–µ–º–ª—è', '—Å–ª–æ–≤–æ', '–Ω–∞–π—Ç–∏', '—Å—Ç–µ–Ω–∫–∞', '–≤–º–µ—Å—Ç–µ', '–≤–∑—è—Ç—å', '—Å–∞–º—ã–π', '—è–π—Ü–æ',\n",
    "                                          '—Å–∫–æ–ª—å–∫–æ', '—Å–º–æ—Ç—Ä–µ—Ç—å', '—Å–¥–µ–ª–∞—Ç—å', '–≥–æ–ª–æ–≤–∞', '–≥–æ–≤–æ—Ä–∏—Ç—å', '–≤–æ–æ–±—â–µ', '–≥–æ–¥',\n",
    "                                          '–¥–µ–Ω—å–≥–∞', '–ø—Ä–æ–¥–∞–∂–Ω—ã–π', '–ø–∏—Å–∞—Ç—å', '—Ä–∞–±–æ—Ç–∞—Ç—å', '–¥—É–º–∞—Ç—å', '–∂–∏–∑–Ω—å', '–º–æ–∑–≥',\n",
    "                                          '—Ä—É—Å—Å–∫–∏–π', '—Å—Ä–∞–∑—É', '–º–∞–ª–æ', '–ø–ª–æ—â–∞–¥—å', '—Å–æ–±–∞–∫–∞', '–µ—Å—Ç–∏', '—Ä–æ—Ç', '—Ö–æ—Ç–µ—Ç—å',\n",
    "                                          '–¥–∞–≤–∞—Ç—å', '–º–∞—Ç—å', '–≤—ã–µ—Å—Ç–∏', '—Å–∏–¥–µ—Ç—å', '–ø–æ–π—Ç–∏', '–¥–∞—Ç—å', '–¥–∞–≤–Ω–æ', '—Å–∞–∂–∞—Ç—å',\n",
    "                                          '–ø–æ–ª–Ω—ã–π', '–ø–æ—Ä–∞', '—Å—Ç–∞—Ç—å', '–¥–æ–ª–∂–Ω—ã–π', '—Å—Ç–∞—Ç—å', '–≤—Ä–µ–º—è', '–ø–æ–∫–∞', '–≤–ª–∞—Å—Ç—å',\n",
    "                                          '–Ω–∏–∫—Ç–æ', '–ø—Ä–∏–≤—è–∑–∞—Ç—å', '–±–æ–≥', '—Å–∫–æ—Ä–æ', '–∫–æ—Ä–º–∏—Ç—å' '–≤—Ä–∞–≥', '—à–µ—è', '–±–∞—à–∫–∞', \n",
    "                                          '–±–∞–±–∞', '–º—É–∂', '–ø–æ–∫–∞–∑–∞—Ç—å', '—É–∫—Ä–∞–∏–Ω–∞', '—Å—Ç–∞—Ä—ã–π', '—Ä–æ–¥–∏—Ç–µ–ª—å', '–ø–æ—Å–∞–¥–∏—Ç—å',\n",
    "                                          '–≤–∏–¥–µ—Ç—å', '–≤—Ä–∞–≥', '—Å—É–ø–µ—Ä', '–∂–µ–Ω—â–∏–Ω–∞']\n",
    "\n",
    "tf_insult = CountVectorizer(max_features = 25, ngram_range = (1, 2), \n",
    "                            stop_words = stopWords).fit(XY.loc[XY.insult == 1, 'text_lemmatised'])\n",
    "tf_threat = CountVectorizer(max_features = 25, ngram_range = (1, 2), \n",
    "                            stop_words = stopWords).fit(XY.loc[XY.threat == 1, 'text_lemmatised'])\n",
    "tf_obscenity = CountVectorizer(max_features = 25, ngram_range = (1, 2), \n",
    "                               stop_words = stopWords).fit(XY.loc[XY.obscenity == 1, 'text_lemmatised'])\n",
    "\n",
    "text_train_insult_2 = (tf_insult.transform(XY.text_lemmatised).toarray() > 0).astype('int')\n",
    "text_test_insult_2 = (tf_insult.transform(X_final_test.text_lemmatised).toarray() > 0).astype('int')\n",
    "\n",
    "text_train_threat_2 = (tf_threat.transform(XY.text_lemmatised).toarray() > 0).astype('int')\n",
    "text_test_threat_2 = (tf_threat.transform(X_final_test.text_lemmatised).toarray() > 0).astype('int')\n",
    "\n",
    "text_train_obscenity_2 = (tf_obscenity.transform(XY.text_lemmatised).toarray() > 0).astype('int')\n",
    "text_test_obscenity_2 = (tf_obscenity.transform(X_final_test.text_lemmatised).toarray() > 0).astype('int')\n",
    "\n",
    "def mini_hyperopt(param):\n",
    "    y_pred = np.array(pred_train_normal_1 >= param['threshold'], dtype = 'int')\n",
    "    return -f1_score(y_pred = y_pred, y_true = XY['normal'])\n",
    "    \n",
    "space_thres = {'threshold': hp.uniform('threshold', 0, 1)}\n",
    "    \n",
    "best_mini = fmin(fn = mini_hyperopt, space = space_thres, algo = tpe.suggest, max_evals = 500)\n",
    "\n",
    "\n",
    "pred_train_normal_bool_1 = np.array(pred_train_normal_1 >= round(best_mini['threshold'], 3), dtype = 'int')\n",
    "pred_test_normal_bool_1 = np.array(pred_test_normal_1 >= round(best_mini['threshold'], 3), dtype = 'int')\n",
    "\n",
    "\n",
    "train_2_insult = np.concatenate([pred_train_insult_1, pred_train_normal_bool_1, text_train_insult_2], \n",
    "                                axis = 1)\n",
    "train_2_threat = np.concatenate([pred_train_threat_1, pred_train_normal_bool_1, text_train_threat_2], \n",
    "                                axis = 1)\n",
    "train_2_obscenity = np.concatenate([pred_train_obscenity_1, pred_train_normal_bool_1, text_train_obscenity_2], \n",
    "                                   axis = 1)\n",
    "\n",
    "test_2_insult = np.concatenate([pred_test_insult_1, pred_test_normal_bool_1, text_test_insult_2], \n",
    "                               axis = 1)\n",
    "test_2_threat = np.concatenate([pred_test_threat_1, pred_test_normal_bool_1, text_test_threat_2], \n",
    "                               axis = 1)\n",
    "test_2_obscenity = np.concatenate([pred_test_obscenity_1, pred_test_normal_bool_1, text_test_obscenity_2], \n",
    "                                  axis = 1)\n",
    "\n",
    "lg_insult = LogisticRegression(**best_insult_C_2).fit(train_2_insult, XY['insult'])\n",
    "lg_threat = LogisticRegression(**best_threat_C_2).fit(train_2_threat, XY['threat'])\n",
    "lg_obscenity = LogisticRegression(**best_obscenity_C_2).fit(train_2_obscenity, XY['obscenity'])\n",
    "\n",
    "pred_test_insult_2 = lg_insult.predict_proba(test_2_insult)[:, 1].reshape(-1, 1)\n",
    "pred_test_threat_2 = lg_threat.predict_proba(test_2_threat)[:, 1].reshape(-1, 1)\n",
    "pred_test_obscenity_2 = lg_obscenity.predict_proba(test_2_obscenity)[:, 1].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "id": "TWbe9_sZ0ONi"
   },
   "outputs": [],
   "source": [
    "predictions = np.hstack([pred_test_normal_1, pred_test_insult_2, pred_test_threat_2, pred_test_obscenity_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['normal', 'insult', 'threat', 'obscenity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "id": "dpk6iVIX0ONj"
   },
   "outputs": [],
   "source": [
    "final_predictions = pd.concat([pd.DataFrame(X_final_test.id.values, columns = ['id']),\n",
    "                               pd.DataFrame(predictions, columns = labels)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>normal</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>obscenity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167315</td>\n",
       "      <td>0.998016</td>\n",
       "      <td>0.023758</td>\n",
       "      <td>0.008704</td>\n",
       "      <td>0.000562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>224546</td>\n",
       "      <td>0.926286</td>\n",
       "      <td>0.038597</td>\n",
       "      <td>0.012242</td>\n",
       "      <td>0.000585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>241309</td>\n",
       "      <td>0.017610</td>\n",
       "      <td>0.999779</td>\n",
       "      <td>0.105649</td>\n",
       "      <td>0.002020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31170</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.999798</td>\n",
       "      <td>0.098300</td>\n",
       "      <td>0.003319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173358</td>\n",
       "      <td>0.957654</td>\n",
       "      <td>0.033564</td>\n",
       "      <td>0.008842</td>\n",
       "      <td>0.000577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99510</th>\n",
       "      <td>192320</td>\n",
       "      <td>0.356521</td>\n",
       "      <td>0.333875</td>\n",
       "      <td>0.085667</td>\n",
       "      <td>0.030779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99511</th>\n",
       "      <td>6646</td>\n",
       "      <td>0.099780</td>\n",
       "      <td>0.999746</td>\n",
       "      <td>0.103001</td>\n",
       "      <td>0.002165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99512</th>\n",
       "      <td>215218</td>\n",
       "      <td>0.874958</td>\n",
       "      <td>0.152845</td>\n",
       "      <td>0.008986</td>\n",
       "      <td>0.000564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99513</th>\n",
       "      <td>139806</td>\n",
       "      <td>0.773577</td>\n",
       "      <td>0.200878</td>\n",
       "      <td>0.009405</td>\n",
       "      <td>0.000571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99514</th>\n",
       "      <td>99052</td>\n",
       "      <td>0.979588</td>\n",
       "      <td>0.025177</td>\n",
       "      <td>0.009258</td>\n",
       "      <td>0.000599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99515 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id    normal    insult    threat  obscenity\n",
       "0      167315  0.998016  0.023758  0.008704   0.000562\n",
       "1      224546  0.926286  0.038597  0.012242   0.000585\n",
       "2      241309  0.017610  0.999779  0.105649   0.002020\n",
       "3       31170  0.000691  0.999798  0.098300   0.003319\n",
       "4      173358  0.957654  0.033564  0.008842   0.000577\n",
       "...       ...       ...       ...       ...        ...\n",
       "99510  192320  0.356521  0.333875  0.085667   0.030779\n",
       "99511    6646  0.099780  0.999746  0.103001   0.002165\n",
       "99512  215218  0.874958  0.152845  0.008986   0.000564\n",
       "99513  139806  0.773577  0.200878  0.009405   0.000571\n",
       "99514   99052  0.979588  0.025177  0.009258   0.000599\n",
       "\n",
       "[99515 rows x 5 columns]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "id": "UbahSrcW0ONk"
   },
   "outputs": [],
   "source": [
    "result = final_predictions.loc[:, ['id', 'normal', 'insult', 'obscenity', 'threat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "id": "jfiODFw10ONm"
   },
   "outputs": [],
   "source": [
    "result.to_csv('result', index = False, header = True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_6 (simple hyperopt).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
