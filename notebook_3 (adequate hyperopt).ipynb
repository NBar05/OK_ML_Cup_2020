{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_lemmatised</th>\n",
       "      <th>normal</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>obscenity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41127</td>\n",
       "      <td>–¥–≤–æ—Ä–Ω–∏–∫–∞ –Ω–∞–¥–æ —Ç–æ–∂–µ —É–Ω–∏—á—Ç–æ–∂–∏—Ç—å!</td>\n",
       "      <td>–¥–≤–æ—Ä–Ω–∏–∫ –Ω–∞–¥–æ —Ç–æ–∂–µ —É–Ω–∏—á—Ç–æ–∂</td>\n",
       "      <td>–¥–≤–æ—Ä–Ω–∏–∫ –Ω–∞–¥–æ —Ç–æ–∂–µ —É–Ω–∏—á—Ç–æ–∂–∏—Ç—å</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6812</td>\n",
       "      <td>–º–æ—è —Å—Ç–∞—Ä—à–∞—è –Ω–µ–¥–µ–ª—é —à–∏–ø–µ–ª–∞, –Ω–µ –ø—Ä–∏–Ω–∏–º–∞–ª–∞ –ø–æ–¥–∫–∏–¥...</td>\n",
       "      <td>–º–æ—è —Å—Ç–∞—Ä—à –Ω–µ–¥–µ–ª —à–∏–ø–µ–ª –Ω–µ –ø—Ä–∏–Ω–∏–º–∞ –ø–æ–¥–∫–∏–¥—ã—à –∫–æ—Ç–æ...</td>\n",
       "      <td>–º–æ–π —Å—Ç–∞—Ä—à–∏–π –Ω–µ–¥–µ–ª—è —à–∏–ø–µ—Ç—å –Ω–µ –ø—Ä–∏–Ω–∏–º–∞—Ç—å –ø–æ–¥–∫–∏–¥—ã...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6256</td>\n",
       "      <td>–ø–æ–ª–Ω–æ—Å—Ç—å—é —Å –≤–∞–º–∏ —Å–æ–≥–ª–∞—Å–Ω–∞!</td>\n",
       "      <td>–ø–æ–ª–Ω–æ—Å—Ç —Å –≤–∞–º —Å–æ–≥–ª–∞—Å–Ω</td>\n",
       "      <td>–ø–æ–ª–Ω–æ—Å—Ç—å—é —Å –≤—ã —Å–æ–≥–ª–∞—Å–Ω—ã–π</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  \\\n",
       "0  41127                     –¥–≤–æ—Ä–Ω–∏–∫–∞ –Ω–∞–¥–æ —Ç–æ–∂–µ —É–Ω–∏—á—Ç–æ–∂–∏—Ç—å!   \n",
       "1   6812  –º–æ—è —Å—Ç–∞—Ä—à–∞—è –Ω–µ–¥–µ–ª—é —à–∏–ø–µ–ª–∞, –Ω–µ –ø—Ä–∏–Ω–∏–º–∞–ª–∞ –ø–æ–¥–∫–∏–¥...   \n",
       "2   6256                         –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å –≤–∞–º–∏ —Å–æ–≥–ª–∞—Å–Ω–∞!   \n",
       "\n",
       "                                        text_stemmed  \\\n",
       "0                          –¥–≤–æ—Ä–Ω–∏–∫ –Ω–∞–¥–æ —Ç–æ–∂–µ —É–Ω–∏—á—Ç–æ–∂   \n",
       "1  –º–æ—è —Å—Ç–∞—Ä—à –Ω–µ–¥–µ–ª —à–∏–ø–µ–ª –Ω–µ –ø—Ä–∏–Ω–∏–º–∞ –ø–æ–¥–∫–∏–¥—ã—à –∫–æ—Ç–æ...   \n",
       "2                              –ø–æ–ª–Ω–æ—Å—Ç —Å –≤–∞–º —Å–æ–≥–ª–∞—Å–Ω   \n",
       "\n",
       "                                     text_lemmatised  normal  threat  insult  \\\n",
       "0                       –¥–≤–æ—Ä–Ω–∏–∫ –Ω–∞–¥–æ —Ç–æ–∂–µ —É–Ω–∏—á—Ç–æ–∂–∏—Ç—å       0       1       0   \n",
       "1  –º–æ–π —Å—Ç–∞—Ä—à–∏–π –Ω–µ–¥–µ–ª—è —à–∏–ø–µ—Ç—å –Ω–µ –ø—Ä–∏–Ω–∏–º–∞—Ç—å –ø–æ–¥–∫–∏–¥—ã...       1       0       0   \n",
       "2                           –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å –≤—ã —Å–æ–≥–ª–∞—Å–Ω—ã–π       1       0       0   \n",
       "\n",
       "   obscenity  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY = pd.read_csv('XY.csv', header = 0)\n",
    "XY.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((104142, 8), (18547, 8))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY_train, XY_test = train_test_split(XY, test_size = 0.3, shuffle = True, random_state = 42)\n",
    "XY_train.reset_index(drop = True, inplace = True)\n",
    "XY_test.reset_index(drop = True, inplace = True)\n",
    "XY_train_abn = XY_train.loc[XY_train.normal == 0, :].reset_index(drop = True)\n",
    "XY_train.shape, XY_train_abn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label predictions\n",
    "## Function manufacture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def search_best_tfidf_and_algo(X, y, ns = 3, num_of_evals = 10):\n",
    "    \n",
    "    def hyperopt_tfidf_algo_score(params):\n",
    "        try:\n",
    "            trans = TfidfVectorizer(stop_words = params['stop_words'], min_df = params['min_df'],\n",
    "                                    ngram_range = params['ngram_range'])\n",
    "            \n",
    "            if params['norm'] == 'lemma':\n",
    "                X_prep = trans.fit_transform(X.text_lemmatised) \n",
    "            elif params['norm'] == 'stem':\n",
    "                X_prep = trans.fit_transform(X.text_stemmed)\n",
    "            else:\n",
    "                X_prep = trans.fit_transform(X.text)\n",
    "            \n",
    "            clf = LogisticRegression(C = params['C'], class_weight = params['class_weight'])\n",
    "\n",
    "            current_score_scores = cross_val_score(clf, X_prep, y, cv = StratifiedKFold(n_splits = ns),\n",
    "                                                   scoring = 'average_precision')\n",
    "            \n",
    "            mean_score = np.mean(current_score_scores)\n",
    "            current_score = mean_score if np.std(current_score_scores * 100) < 1.5 else mean_score / 1.5\n",
    "            \n",
    "        except:\n",
    "            current_score = 0\n",
    "            print(f\"Bad min_df: {algo_params['min_df']}\")\n",
    "        \n",
    "        return -current_score\n",
    "        \n",
    "    space_tfidf = {\n",
    "        'min_df': hp.choice('min_df', np.arange(2, 8, 1)),\n",
    "        'stop_words': hp.choice('stop_words', [stopWords, None]),\n",
    "        'ngram_range': hp.choice('ngram_range', [(1, 1), (1, 2)]),\n",
    "        'norm': hp.choice('norm', ['lemma', 'stem', 'initial'])\n",
    "    }\n",
    "    \n",
    "    space_algo = {'C': hp.uniform('C', 10**(0), 10**(2)), \n",
    "                  'class_weight': hp.choice('class_weight', ['balanced', None])}\n",
    "    \n",
    "    space = dict(**space_tfidf, **space_algo)\n",
    "    \n",
    "    best = fmin(fn = hyperopt_tfidf_algo_score, space = space, algo = tpe.suggest, max_evals = num_of_evals)\n",
    "    \n",
    "    best['min_df'] = np.arange(1, 6, 1)[best['min_df']]\n",
    "    best['stop_words'] = [stopWords, None][best['stop_words']]\n",
    "    best['ngram_range'] = [(1, 1), (1, 2)][best['ngram_range']]\n",
    "    \n",
    "    best['norm'] = ['lemma', 'stem', 'initial'][best['norm']]\n",
    "    \n",
    "    best['class_weight'] = ['balanced', None][best['class_weight']]\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def build_best_tfidf_and_algo(params, X_train, y_train, X_valid, y_valid):\n",
    "    \n",
    "    trans = TfidfVectorizer(min_df = params['min_df'], stop_words = params['stop_words'], \n",
    "                            ngram_range = params['ngram_range'])\n",
    "    \n",
    "    if params['norm'] == 'lemma':\n",
    "        X_train_prep = trans.fit_transform(X_train.text_lemmatised)\n",
    "        X_valid_prep = trans.transform(X_valid.text_lemmatised)\n",
    "    elif params['norm'] == 'stem':\n",
    "        X_train_prep = trans.fit_transform(X_train.text_stemmed)\n",
    "        X_valid_prep = trans.transform(X_valid.text_stemmed)\n",
    "    else:\n",
    "        X_train_prep = trans.fit_transform(X_train.text)\n",
    "        X_valid_prep = trans.transform(X_valid.text)\n",
    "    \n",
    "    clf = LogisticRegression(C = params['C'], class_weight = params['class_weight'])\n",
    "    clf.fit(X_train_prep, y_train)\n",
    "    \n",
    "    aps_train = average_precision_score(y_true = y_train, y_score = clf.predict_proba(X_train_prep)[:, 1])\n",
    "    aps_valid = average_precision_score(y_true = y_valid, y_score = clf.predict_proba(X_valid_prep)[:, 1])\n",
    "    \n",
    "    return {'norm': params['norm'], 'transformer': trans, 'classifier': clf, 'scores': [aps_train, aps_valid]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_build(XY, label, balancing = True, test_size = 0.2, ns = 5, num_of_evals = 10):\n",
    "    \n",
    "    if balancing:\n",
    "        sample_num = min(XY.loc[XY[label] == 1].shape[0], XY.loc[XY[label] == 0].shape[0])\n",
    "        XY_balanced = pd.concat([XY.loc[XY[label] == 1].sample(sample_num, random_state = 42),\n",
    "                                 XY.loc[XY[label] == 0].sample(sample_num, random_state = 42)], axis = 0)\n",
    "    else:\n",
    "        XY_balanced = XY\n",
    "    \n",
    "    XY_train, XY_valid = train_test_split(XY_balanced, stratify = XY_balanced[label], test_size = test_size,\n",
    "                                          shuffle = True, random_state = 42)\n",
    "    XY_train.reset_index(drop = True, inplace = True)\n",
    "    XY_valid.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    X_train = XY_train.drop(columns = label)\n",
    "    y_train = XY_train.loc[:, label].values\n",
    "    \n",
    "    X_valid = XY_valid.drop(columns = label)\n",
    "    y_valid = XY_valid.loc[:, label].values\n",
    "    \n",
    "    params = search_best_tfidf_and_algo(X_train, y_train, ns = ns, num_of_evals = num_of_evals)\n",
    "    \n",
    "    return build_best_tfidf_and_algo(params, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monster_search(XY: pd.DataFrame, labels: list = ['normal'], balancing: list = [True], test_size: float = 0.2, \n",
    "                   ns: list = [10], num_of_evals: list = [10], final_algos: dict = {}) -> dict:\n",
    "    \n",
    "    for l, label in enumerate(labels):\n",
    "        print()\n",
    "        print('---------------------------------------------------------------------------------------------------')\n",
    "        print(label)\n",
    "        print('---------------------------------------------------------------------------------------------------')\n",
    "        \n",
    "        algo_result = search_build(XY, label, balancing = balancing[l], test_size = test_size, \n",
    "                                   ns = ns[l], num_of_evals = num_of_evals[l])\n",
    "        \n",
    "        final_algos[label] = algo_result\n",
    "    \n",
    "    return final_algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "normal\n",
      "---------------------------------------------------------------------------------------------------\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [05:49<00:00,  6.99s/trial, best loss: -0.9911795125105891]\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "insult\n",
      "---------------------------------------------------------------------------------------------------\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [06:14<00:00,  7.49s/trial, best loss: -0.8936379998879065]\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "obscenity\n",
      "---------------------------------------------------------------------------------------------------\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [05:27<00:00,  6.54s/trial, best loss: -0.7525734737778982] \n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "threat\n",
      "---------------------------------------------------------------------------------------------------\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [05:43<00:00,  6.88s/trial, best loss: -0.8632498774249159]\n"
     ]
    }
   ],
   "source": [
    "final_algos_1 = monster_search(XY_train, labels = ['normal', 'insult', 'obscenity', 'threat'],\n",
    "                               balancing = [False] * 4, test_size = 0.2, ns = [5] * 4, num_of_evals = [250] * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_algos_1\n",
    "# {'normal': {'norm': 'stem',\n",
    "#   'transformer': TfidfVectorizer(min_df=3, ngram_range=(1, 2)),\n",
    "#   'classifier': LogisticRegression(C=43.61225142163715, class_weight='balanced'),\n",
    "#   'scores': [0.9997501061237812, 0.9906656653390087]},\n",
    "#  'insult': {'norm': 'stem',\n",
    "#   'transformer': TfidfVectorizer(min_df=4, ngram_range=(1, 2), stop_words=stopWords),\n",
    "#   'classifier': LogisticRegression(C=14.163874788479188),\n",
    "#   'scores': [0.9703978115236113, 0.9018463501179763]},\n",
    "#  'obscenity': {'norm': 'initial',\n",
    "#   'transformer': TfidfVectorizer(min_df=2),\n",
    "#   'classifier': LogisticRegression(C=41.88942360620957),\n",
    "#   'scores': [0.9920524059927307, 0.752306287824738]},\n",
    "#  'threat': {'norm': 'initial',\n",
    "#   'transformer': TfidfVectorizer(min_df=10, ngram_range=(1, 2)),\n",
    "#   'classifier': LogisticRegression(C=2.047808035445657),\n",
    "#   'scores': [0.9337172577508484, 0.8503346208600301]}}\n",
    "\n",
    "{'normal': {'classifier': LogisticRegression(C = 9, class_weight = 'balanced'),\n",
    "            'norm': 'stem',\n",
    "            'scores': [0.9998298284695576, 0.9929094934296164],\n",
    "            'transformer': TfidfVectorizer(min_df = 1, ngram_range = (1, 1))},\n",
    " 'insult': {'classifier': LogisticRegression(C = 10, class_weight = None),\n",
    "            'norm': 'initial',\n",
    "            'scores': [0.9866985845174877, 0.9050294725010097],\n",
    "            'transformer': TfidfVectorizer(min_df = 2, ngram_range = (1, 1))},\n",
    " 'obscenity': {'classifier': LogisticRegression(C = 94, class_weight = None),\n",
    "               'norm': 'lemma',\n",
    "               'scores': [0.9901362947613137, 0.7137456145906628],\n",
    "               'transformer': TfidfVectorizer(min_df = 2, ngram_range = (1, 2), stop_words = stopWords)},\n",
    " 'threat': {'classifier': LogisticRegression(C = 14, class_weight = None),\n",
    "            'norm': 'initial',\n",
    "            'scores': [0.9896822419756792, 0.8677644706078192],\n",
    "            'transformer': TfidfVectorizer(min_df = 3, ngram_range = (1, 1))}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_predictions(data, algos):\n",
    "    \n",
    "    if algos['normal']['norm'] == 'lemma':\n",
    "        X_train_n = algos['normal']['transformer'].transform(data.text_lemmatised)\n",
    "    elif algos['normal']['norm'] == 'stem':\n",
    "        X_train_n = algos['normal']['transformer'].transform(data.text_stemmed)\n",
    "    else:\n",
    "        X_train_n = algos['normal']['transformer'].transform(data.text)\n",
    "    \n",
    "    preds = algos['normal']['classifier'].predict_proba(X_train_n)[:, 1].reshape(-1, 1)\n",
    "    predicted_labels_n = np.abs(algos['normal']['classifier'].predict(X_train_n) - 1).reshape(-1, 1)\n",
    "    \n",
    "    for label, algo in [(key, value) for key, value in algos.items() if key != 'normal']:\n",
    "        \n",
    "        if algo['norm'] == 'lemma':\n",
    "            X_train_abn = algo['transformer'].transform(data.text_lemmatised)\n",
    "        elif algo['norm'] == 'stem':\n",
    "            X_train_abn = algo['transformer'].transform(data.text_stemmed)\n",
    "        else:\n",
    "            X_train_abn = algo['transformer'].transform(data.text)\n",
    "        \n",
    "        preds_abn = algo['classifier'].predict_proba(X_train_abn)[:, 1].reshape(-1, 1) #* predicted_labels_n\n",
    "        preds = np.concatenate([preds, preds_abn], axis = 1)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# predictions = get_prob_predictions(XY_train, final_algos_1)\n",
    "# average_precision_score(y_true = XY_train.loc[:, list(final_algos_1.keys())],\n",
    "#                         y_score = predictions, average = 'macro') # 0.9565494854057154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = get_prob_predictions(XY_test, final_algos_1)\n",
    "# average_precision_score(y_true = XY_test.loc[:, list(final_algos_1.keys())], \n",
    "#                         y_score = predictions, average = 'macro') # 0.8762971108666133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_lemmatised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167315</td>\n",
       "      <td>–∫–∞–∫–∞—è –ø—Ä–µ–ª–µ—Å—Ç—å!!!üòç</td>\n",
       "      <td>–∫–∞–∫–∞—è –ø—Ä–µ–ª–µ—Å—Ç—åüòç</td>\n",
       "      <td>–∫–∞–∫–æ–π –ø—Ä–µ–ª–µ—Å—Ç—åüòç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>224546</td>\n",
       "      <td>–∫–∞–∞–ª –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤—å—é?</td>\n",
       "      <td>–∫–∞–∞ –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤</td>\n",
       "      <td>–∫–∞–∞–ª–∞ –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤—å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>241309</td>\n",
       "      <td>–≥–Ω–æ–π–Ω—ã–µ –ø–∏–¥–æ—Ä—ã –∞–ª–ª—ã –æ–Ω–∏</td>\n",
       "      <td>–≥–Ω–æ–π–Ω –ø–∏–¥–æ—Ä –∞–ª–ª –æ–Ω–∏</td>\n",
       "      <td>–≥–Ω–æ–π–Ω—ã–π –ø–∏–¥–æ—Ä –∞–ª–ª–∞ –æ–Ω–∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31170</td>\n",
       "      <td>—á—ë —Ç—ã –≥—É–±—ã —à–ª—ë—à—å –≤ –ø–æ–º–∞–¥–µ?—Ñ—É –±–ª—è–¥—å</td>\n",
       "      <td>—á–µ —Ç—ã –≥—É–± —à–ª–µ—à –≤ –ø–æ–º–∞–¥–µ—Ñ –±–ª—è–¥</td>\n",
       "      <td>—á—Ç–æ —Ç—ã –≥—É–±–∞ —Å–ª–∞—Ç—å –≤ –ø–æ–º–∞–¥–µ—Ñ–∞ –±–ª—è–¥—å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173358</td>\n",
       "      <td>–º–∞—Ç—Ä–æ–Ω–∞ –ø–æ–º–æ–≥–∞–µ—Ç —Ä–µ–∞–ª—å–Ω–æ —ç—Ç–æ –ø—Ä–∞–≤–¥–∞. —Å–∞–º–∞ –∫ –Ω–µ...</td>\n",
       "      <td>–º–∞—Ç—Ä–æ–Ω –ø–æ–º–æ–≥–∞ —Ä–µ–∞–ª—å–Ω —ç—Ç –ø—Ä–∞–≤–¥ —Å–∞–º –∫ –Ω–µ–π –µ–∑–¥ –Ω–∞...</td>\n",
       "      <td>–º–∞—Ç—Ä–æ–Ω–∞ –ø–æ–º–æ–≥–∞—Ç—å —Ä–µ–∞–ª—å–Ω–æ —ç—Ç–æ –ø—Ä–∞–≤–¥–∞ —Å–∞–º –∫ –æ–Ω–∞ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  \\\n",
       "0  167315                                 –∫–∞–∫–∞—è –ø—Ä–µ–ª–µ—Å—Ç—å!!!üòç   \n",
       "1  224546                            –∫–∞–∞–ª –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤—å—é?   \n",
       "2  241309                            –≥–Ω–æ–π–Ω—ã–µ –ø–∏–¥–æ—Ä—ã –∞–ª–ª—ã –æ–Ω–∏   \n",
       "3   31170                 —á—ë —Ç—ã –≥—É–±—ã —à–ª—ë—à—å –≤ –ø–æ–º–∞–¥–µ?—Ñ—É –±–ª—è–¥—å   \n",
       "4  173358  –º–∞—Ç—Ä–æ–Ω–∞ –ø–æ–º–æ–≥–∞–µ—Ç —Ä–µ–∞–ª—å–Ω–æ —ç—Ç–æ –ø—Ä–∞–≤–¥–∞. —Å–∞–º–∞ –∫ –Ω–µ...   \n",
       "\n",
       "                                        text_stemmed  \\\n",
       "0                                    –∫–∞–∫–∞—è –ø—Ä–µ–ª–µ—Å—Ç—åüòç   \n",
       "1                                –∫–∞–∞ –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤   \n",
       "2                                –≥–Ω–æ–π–Ω –ø–∏–¥–æ—Ä –∞–ª–ª –æ–Ω–∏   \n",
       "3                      —á–µ —Ç—ã –≥—É–± —à–ª–µ—à –≤ –ø–æ–º–∞–¥–µ—Ñ –±–ª—è–¥   \n",
       "4  –º–∞—Ç—Ä–æ–Ω –ø–æ–º–æ–≥–∞ —Ä–µ–∞–ª—å–Ω —ç—Ç –ø—Ä–∞–≤–¥ —Å–∞–º –∫ –Ω–µ–π –µ–∑–¥ –Ω–∞...   \n",
       "\n",
       "                                     text_lemmatised  \n",
       "0                                    –∫–∞–∫–æ–π –ø—Ä–µ–ª–µ—Å—Ç—åüòç  \n",
       "1                             –∫–∞–∞–ª–∞ –∫–∞–∫–æ–π –Ω–µ —Å –∫—Ä–æ–≤—å  \n",
       "2                             –≥–Ω–æ–π–Ω—ã–π –ø–∏–¥–æ—Ä –∞–ª–ª–∞ –æ–Ω–∏  \n",
       "3                 —á—Ç–æ —Ç—ã –≥—É–±–∞ —Å–ª–∞—Ç—å –≤ –ø–æ–º–∞–¥–µ—Ñ–∞ –±–ª—è–¥—å  \n",
       "4  –º–∞—Ç—Ä–æ–Ω–∞ –ø–æ–º–æ–≥–∞—Ç—å —Ä–µ–∞–ª—å–Ω–æ —ç—Ç–æ –ø—Ä–∞–≤–¥–∞ —Å–∞–º –∫ –æ–Ω–∞ ...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final_test = pd.read_csv('X_final_test.csv', header = 0)\n",
    "X_final_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = get_prob_predictions(X_final_test, final_algos_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_frame = pd.concat([pd.DataFrame(X_final_test.id.values, columns = ['id']), \n",
    "                         pd.DataFrame(predictions, columns = list(final_algos_1.keys()))], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final_frame.loc[:, ['id', 'normal', 'insult', 'obscenity', 'threat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('result', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predictions = get_prob_predictions(XY, loaded_model, activates_s)\n",
    "# average_precision_score(y_true = XY.loc[:, ['normal', 'insult', 'threat', 'obscenity']],\n",
    "#                         y_score = predictions, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(final_algos, open('final_algos', 'wb'))\n",
    "# pickle.dump(activates_s, open('activates_s', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_models = pickle.load(open('final_algos', 'rb'))\n",
    "# loaded_activates_s = pickle.load(open('activates_s', 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
